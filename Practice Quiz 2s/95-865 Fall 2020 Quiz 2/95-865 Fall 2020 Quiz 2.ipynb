{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 95-865 Fall 2020 Quiz 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your name:** \n",
    "\n",
    "**Your Andrew ID:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Conceptual Questions (No coding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A video streaming company Hooloo wants to understand which users are likely to subscribe\n",
    "to its monthly subscription service. They hire you to analyze their data. Hooloo specifically is\n",
    "interested in understanding how users' preferences in different genres are associated with\n",
    "whether they end up subscribing after an initial trial period (in which users watch whatever they\n",
    "want on the streaming service for free for up to 7 days).\n",
    "\n",
    "To tackle this problem, you decide to use topic modeling combined with classification. The idea\n",
    "is that users can be represented as being partly interested in different genres (which are the\n",
    "topics). For the purposes of this problem, assume that an LDA topic model has already been\n",
    "learned for 5 topics and that the topics are interpretable: it turns out that:\n",
    "\n",
    "topic 0 corresponds to\"action\", \n",
    "topic 1 to \"comedy\", \n",
    "topic 2 to \"drama\", \n",
    "topic 3 to \"romance\", \n",
    "and topic 4 to \"science fiction\".\n",
    "\n",
    "For example, a user with 20% interest in \"action\", 5% interest in \"comedy\", 35% interest in\n",
    "\"romance\", and 40% interest in \"science fiction\" could be represented as the \"LDA-transformed\"\n",
    "feature vector [0.2, 0.05, 0.0, 0.35, 0.4] .\n",
    "\n",
    "For an arbitrary user, we denote the user's LDA-transformed feature vector representation by\n",
    "the variable z , which is a 1D numpy array of numbers of length 5 (since there are 5 topics). To\n",
    "make a prediction for whether the user corresponding to z will subscribe to the Hooloo\n",
    "subscription service, we further assume that we feed z as input to a logistic regression\n",
    "predictor, i.e., a linear layer with 2 nodes and softmax activation. Suppose that this already-\n",
    "trained linear layer has the 5-by-2 weight matrix given by\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1/2 & 0 \\\\\n",
    "0 & 1/2 \\\\\n",
    "0 & 1/2 \\\\\n",
    "1/2 & 0 \\\\\n",
    "0 & 0\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "and bias vector [-1/2, 1/2]. Note that here, output node 0 corresponds to \"will subscribe\" and\n",
    "output node 1 corresponds to \"will not subscribe\". Also, for the weight matrix, the rows\n",
    "correspond to topics 0, 1, 2, 3, and 4. The weight matrix and bias vector at this point are treated\n",
    "as fixed---we are not updating the values in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) [5 points]** Hooloo wants to ensure that among users predicted to be likely to subscribe, that\n",
    "as many of these users are actually going to subscribe. Which metric do they care about?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- True positive rate\n",
    "- False positive rate\n",
    "- Precision\n",
    "- F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here (for this part, we are explicitly not asking you to explain your answer and will only be giving full credit for a correct answer or 0 for an incorrect answer):** REPLACE THIS TEXT WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) [5 points]** True or false: if we add up the values in z , we get 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- True\n",
    "- False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here (for this part, we are explicitly not asking you to explain your answer and will only be giving full credit for a correct answer or 0 for an incorrect answer):** REPLACE THIS TEXT WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) [5 points]** According to the already trained logistic regression model, which two genres\n",
    "being present are associated with a user being more likely to subscribe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- action\n",
    "- comedy\n",
    "- drama\n",
    "- romance\n",
    "- science fiction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here (for this part, we are explicitly not asking you to explain your answer and will only be giving full credit for a correct answer or 0 for an incorrect answer):** REPLACE THIS TEXT WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) [5 points]** Does the \"science ﬁction\" topic aﬀect the prediction of the already trained logistic\n",
    "regression model?\n",
    "- Yes\n",
    "- No\n",
    "\n",
    "**Your answer here:** REPLACE THIS TEXT WITH YOUR ANSWER\n",
    "\n",
    "**Briefly justify your answer (for this part, a correct answer without an explanation will not receive full credit):** REPLACE THIS TEXT WITH YOUR ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) [5 points]** By feeding `z` into the already trained logistic regression model, what is the output\n",
    "before the softmax activation? Note that this output has 2 values (1 per node). For each value,\n",
    "please write the value in terms of the variable `z` . For example, if you think output node 0 has a\n",
    "value given by the 0-th entry of `z` added to the log of the 4-th entry of `z` , then you would write\n",
    "`z[0] + log(z[4])`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output node 0 (\"will subscribe\") value before softmax (just the ﬁnal answer; we ask for an\n",
    "explanation later):** REPLACE THIS TEXT WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output node 1 (\"will subscribe\") value before softmax (just the ﬁnal answer; we ask for an\n",
    "explanation later):** REPLACE THIS TEXT WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Brieﬂy explain how you got your answers above (correct answers above without correct explanations will not receive full credit):**\n",
    "    \n",
    "- Output node 0: REPLACE THIS TEXT WITH YOUR ANSWER\n",
    "\n",
    "- Output node 1: REPLACE THIS TEXT WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f) [5 points]** Continuing oﬀ the previous subpart, by feeding `z` into the already trained logistic\n",
    "regression model, what is the ﬁnal output, i.e., the output after the softmax activation? Just as\n",
    "with **part (e)**, please express your answer in terms of the variable `z` ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output node 0 (\"will subscribe\") value after softmax (just the ﬁnal answer; we ask for an\n",
    "explanation later):** REPLACE THIS TEXT WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output node 1 (\"will subscribe\") value after softmax (just the ﬁnal answer; we ask for an\n",
    "explanation later):** REPLACE THIS TEXT WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Brieﬂy explain how you got your answers above (correct answers above without correct explanations will not receive full credit):** REPLACE THIS TEXT WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g) [5 points]** True or false: when making a prediction, we do not actually need to compute the\n",
    "softmax activation; we can already ﬁgure out which class has higher probability by only using\n",
    "the linear layer output before the softmax activation. In other words, to ﬁgure out which class\n",
    "has higher probability, we do not actually need to do the calculation in part (f).\n",
    "- True\n",
    "- False\n",
    "\n",
    "**Your answer here:** REPLACE THIS TEXT WITH YOUR ANSWER\n",
    "\n",
    "**Briefly justify your answer (for this part, a correct answer without an explanation will not receive full credit):** REPLACE THIS TEXT WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Predicting Alcoholism Using Brain Activity Data (65 Points)\n",
    "\n",
    "Before starting this problem, please be sure to download the dataset:\n",
    "\n",
    "http://www.andrew.cmu.edu/user/georgech/95-865/Fall%202020%20quiz%202%20data.zip\n",
    "\n",
    "In this problem, we look at how brain activity can be used to predict whether a person is an\n",
    "alcoholic or not. Here, brain activity is measured using electroencephalogram (EEG) sensors.\n",
    "For the purposes of this problem, you don't need to know how EEG works. We've computed a\n",
    "preprocessed dataset, where each data point is for one person and consists of a time series of\n",
    "length 256 (there are 256 time steps, where at each time step there is 1 measurement; each ).\n",
    "You can think of a higher value for the time series to mean more brain activity and a smaller\n",
    "value to be less brain activity. For the code in this problem, 1 corresponds to \"alcoholic\"\n",
    "whereas 0 corresponds to \"non-alcoholic\".\n",
    "\n",
    "In your Jupyter notebook answer booklet, be sure to run the next cell ﬁrst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL -- ** BE SURE TO RUN THIS CELL **\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "# the next two lines are needed on my Intel-based MacBook Air to get the code to run; you likely don't need t\n",
    "# (in fact I used to not need these two lines)\n",
    "# import os\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummaryX import summary\n",
    "from UDA_pytorch_utils import UDA_LSTMforSequential, UDA_pytorch_classifier_fit, \\\n",
    "UDA_plot_train_val_accuracy_vs_epoch, UDA_pytorch_classifier_predict, \\\n",
    "UDA_compute_accuracy, UDA_pytorch_model_transform\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) [5 points]** Let's ﬁrst make sure that you can load in the data and plot what one of the EEG\n",
    "time series looks like. Please complete the following tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the .txt data with `np.loadtxt(filename)` .\n",
    "- Report the shape of training set features and test set features.\n",
    "- Plot the 0th sample feature in the training set (the x-axis should correspond to the diﬀerent time steps 0, 1, ..., up to 255, and the y-axis should be the brain activity measurement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL -- ** BE SURE TO RUN THIS CELL **\n",
    "train_features = np.loadtxt('EEG_single_channel_train_features.txt')\n",
    "test_features = np.loadtxt('EEG_single_channel_test_features.txt')\n",
    "train_labels = np.loadtxt('EEG_single_channel_train_labels.txt')\n",
    "test_labels = np.loadtxt('EEG_single_channel_test_labels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL -- ** BE SURE TO RUN THIS CELL **\n",
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL -- ** BE SURE TO RUN THIS CELL **\n",
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# WRITE YOUR CODE HERE (PLOTTING DATA POINT 0 TIME SERIES)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) [14 points across subparts]** Let's try doing the prediction with a simple baseline model first. Specifically, let's use a **random forest**, treating each time series as a feature vector (so basically we do not actually take advantage of time series structure).\n",
    "\n",
    "\n",
    "**Subpart i. [3 points]** Fit the model with sklearn's random forest classifier; set `max_features` to 15, `random_state` to 0, and leave all other hyperparameters to their default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# WRITE YOUR CODE HERE (TRAINING RANDOM FOREST)\n",
    "#\n",
    "rf = None  # this should be a RandomForestClassifier instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subpart ii. [4 points]** Report the training and test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# WRITE YOUR CODE HERE (COMPUTING TRAINING AND TESTING ACCURACY)\n",
    "#\n",
    "rf_predicted_test_labels = None  # to be filled out\n",
    "# be sure to print training and test set accuracy values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subpart iii. [3 points]** Plot the confusion matrix of the test result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# WRITE YOUR CODE HERE (PLOTTING THE CONFUSION MATRIX)\n",
    "# (hint: use `plot_confusion_matrix` (already imported for you))\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subpart iv. [4 points]** Plot the ROC curve with calculated AUC on the test result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# WRITE YOUR CODE HERE (ROC CURVE)\n",
    "# (hint: use `auc` and/or `roc_curve` (already imported for you))\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) [28 points across subparts]** Now we try using a CNN. Specifically, we represent each training point as a 2D grayscale image, where the rows correspond to the time steps, and there is only 1 column.\n",
    "\n",
    "Throughout this part, we use the following sequential CNN architecture:\n",
    "\n",
    "1. A convolution layer with **16 kernels**, each of which is of size **2 rows by 1 column** (please figure out the number of **input** channel(s) by yourself)\n",
    "2. ReLU activation function\n",
    "3. A Maxpooling layer with kernel size **2 rows by 1 column**;\n",
    "4. A convolution layer with **8 kernels**, each of which is of size **5 rows by 1 column** (please figure out the number of **input** channel(s) by yourself)\n",
    "5. ReLU activation function\n",
    "6. A convolution layer with **8 kernels**, each of which is of size **5 rows by 1 column** (please figure out the number of **input** channel(s) by yourself)\n",
    "7. A Maxpooling layer with kernel size **5 rows by 1 column**;\n",
    "8. ReLU activation function\n",
    "9. A Flatten layer\n",
    "10. A Fully Connected layer with **2 output features**\n",
    "11. Softmax activation for classification\n",
    "\n",
    "**Subpart i. [5 points]**  Write code that constructs a PyTorch model implementing the above CNN **and also** reports of a summary table of the CNN with the example input `torch.zeros((200, ?, 256, 1))` (where you have to replace `?` with the  correct number of channels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# DO NOT MODIFY THIS TOP BLOCK OF CODE\n",
    "#\n",
    "torch.manual_seed(0) # DO NOT MODIFY\n",
    "np.random.seed(0) # DO NOT MODIFY\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# WRITE YOUR CODE HERE (CONSTRUCT CNN)\n",
    "#\n",
    "convnet = None  # this should be a nn.Sequential model\n",
    "# be sure to print a summary\n",
    "\n",
    "\n",
    "\n",
    "# NOTE: later in subpart iv, you will want to copy and paste code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subpart ii. [2 points]** Briefly explain why the total number of parameters of the second convolution layer (layer #4 in the CNN architecture description above) is the number shown in the summary.\n",
    "\n",
    "**Your answer here along with an brief justification (for this part, a correct answer without an explanation will not receive full credit):** REPLACE THIS TEXT WITH YOUR ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subpart iii. [2 points]** Briefly explain why the total number of parameters of the last fully connected layer (layer #10 in the CNN architecture description above) is the number shown in the summary.\n",
    "\n",
    "**Your answer here along with an brief justification (for this part, a correct answer without an explanation will not receive full credit):** REPLACE THIS TEXT WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subpart iv. [13 points]** We now train the CNN. First, write code to split the training set into a proper training set and a validation set. Then train the CNN with the Adam optimizer using three different learning rates: 1e-1, 1e-3, and 1e-5 (do **not** tune any other hyperparameters aside from the learning rate; for simplicity, do not try other learning rates aside from the three we're asking you to try). **We suggest that you copy and paste this block of code to be in the same Jupyter notebook cell as the code for subpart i for debugging purposes (so that re-running the cell will indeed retrain the model from scratch rather than resuming training from an intermediate model).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# DO NOT MODIFY THIS TOP BLOCK OF CODE\n",
    "#\n",
    "num_epochs = 50 # DO NOT MODIFY\n",
    "batch_size = 40 # DO NOT MODIFY\n",
    "proper_train_size = int(0.6 * len(train_labels)) # DO NOT MODIFY\n",
    "val_size = len(train_labels) - proper_train_size # DO NOT MODIFY\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# WRITE YOUR CODE HERE (COMPUTE PROPER TRAINING/VALIDATION SETS,\n",
    "# SET LEARNING RATE)\n",
    "#\n",
    "proper_train_dataset = []\n",
    "val_dataset = []\n",
    "learning_rate = -1\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# DO NOT MODIFY THE CODE BELOW\n",
    "#\n",
    "train_accuracies, val_accuracies = UDA_pytorch_classifier_fit(convnet,\n",
    "                                                              torch.optim.Adam(convnet.parameters(),\n",
    "                                                                               lr=learning_rate),#includes softmax\n",
    "                                                              nn.CrossEntropyLoss(), \n",
    "                                                              proper_train_dataset,\n",
    "                                                              val_dataset,\n",
    "                                                              num_epochs, batch_size)\n",
    "UDA_plot_train_val_accuracy_vs_epoch(train_accuracies,val_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subpart iv. [13 points]** Of the three learning rates we asked you to try (1e-1, 1e-3, 1e-5), which would you choose to use, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answer here (no coding):** REPLACE THIS TEXT WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subpart v. [3 points]** Using the best learning rate (among the three we asked you to try) that you chose in the previous part, compute the test set raw accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# WRITE YOUR CODE HERE (TEST SET RAW ACCURACY)\n",
    "#\n",
    "cnn_predicted_test_labels = None  # to be filled out\n",
    "# be sure to print the test set raw accuracy"
   ]
  },
  {
   "attachments": {
    "mystery_donkey.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subpart vi. [3 points]** Interpretation. We separately also trained the same model (remember that on different machines, even if you set a random seed for PyTorch, it is possible to get different answers!). We plot the values of the 16 kernels in the first convolution layer as follows (you do not need to compare to see whether your first convolution layer's kernels have the same values; they will likely not have the same values, which is fine). In our plot below, each row is a kernel/filter with two values. For example, row 0 corresponds to the 0-th filter, which has a value somewhat close to 0 followed by a value somewhat close to 0.4. (In case you are having trouble viewing the image below, we have also included it with the quiz as the file `mystery_donkey.png`.)\n",
    "\n",
    "![mystery_donkey.png](attachment:mystery_donkey.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which kernel has two weights with the biggest difference？What kind of time series trend will activate this kernel？\n",
    "**Hint:** think about the edge detection for images.\n",
    "\n",
    "**Your answer here along with an brief justification (for this part, a correct answer without an explanation will not receive full credit):** REPLACE THIS TEXT WITH YOUR ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) [13 points across subparts]** Finally, we try using a recurrent neural net with the following layers:\n",
    "\n",
    "1. An LSTM layer with **output size 12**\n",
    "2. ReLU activation function\n",
    "3. A Fully Connected layer with **2 output features** (please figure out the number of **input** feature(s) by yourself)\n",
    "4. Softmax activation for classification\n",
    "\n",
    "**Subpart i. [5 points]** Write code to construct a PyTorch model implementing the recurrent neural net stated above and report the summary table with the input `torch.zeros((200, 19, 1))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0) # DO NOT MODIFY\n",
    "np.random.seed(0) # DO NOT MODIFY\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# WRITE YOUR CODE HERE (CONSTRUCT RNN AND REPORT SUMMARY)\n",
    "#\n",
    "rnn = None  # this should be an nn.Sequential model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the number 19 mean for the example input above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here (for this subpart, we are explicitly grading just based on whether your answer is correct or not, so no explanation is needed; you either get full credit or no credit):** REPLACE THIS TEXT WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subpart ii. [5 points]** Train the RNN using the Adam optimizer with a learning rate of 1e-2, a batch size of 20, and a total of 20 epochs. You don't need to tune the hyperparameters and don't worry about the accuracy of the model (if you find it to be unusually low or high--the point of this problem is **not** for you to be hyperparameter tuning to figure out how to get the RNN to get as high of an accuracy as possible; we just want to see that you can get the RNN to train). **To get training to work, you should only need to fill in the portion below on computing the proper training/validation sets (for simplicity, use the same fraction of data in the proper training set as when you trained the CNN--either use a new random split for what data are selected as proper training data vs validation data, or reuse the indices from when you trained the CNN for what is proper training vs validation). We've already put in code for you that does the model training using learning rate 1e-2, batch size 20, and number of epochs 20.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# WRITE YOUR CODE HERE (COMPUTE PROPER TRAINING/VALIDATION SETS)\n",
    "#\n",
    "proper_train_dataset = []\n",
    "val_dataset = []\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# DO NOT MODIFY THE CODE BELOW\n",
    "#\n",
    "train_accuracies, val_accuracies = \\\n",
    "    UDA_pytorch_classifier_fit(rnn,\n",
    "                               torch.optim.Adam(rnn.parameters(),\n",
    "                                                lr=1e-2),\n",
    "                               nn.CrossEntropyLoss(),  # includes softmax\n",
    "                               proper_train_dataset, val_dataset,20,20)\n",
    "UDA_plot_train_val_accuracy_vs_epoch(train_accuracies, val_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subpart iii. [3 points]** Report the test set raw accuracy for the RNN model you trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# WRITE YOUR CODE HERE (TEST SET RAW ACCURACY)\n",
    "#\n",
    "rnn_predicted_test_labels = None  # to be filled out\n",
    "# be sure to print the test set raw accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) [5 points]** Of the random forest, CNN, and RNN models that you trained, which has the highest **precision** on the test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here along with code to justify your answer (for this part, a correct answer without code that justifies your answer will not receive full credit):** REPLACE THIS TEXT WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# WRITE YOUR CODE HERE THAT HELPS JUSTIFY YOUR ANSWER\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
