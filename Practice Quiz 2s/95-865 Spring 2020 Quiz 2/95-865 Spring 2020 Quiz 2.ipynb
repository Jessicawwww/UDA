{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 95-865 Spring 2020 Quiz 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your name:**\n",
    "\n",
    "**Your Andrew ID:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Neural Net Concepts (no coding) [40 points]\n",
    "\n",
    "Parts **(a)** and **(b)** can be done in either order. Part **(c)** (which is very short) only depends on the introductions to parts **(a)** and **(b)**. Part **(d)** (which is also very short) depends on you completing both parts **(a)** and **(b)** first.\n",
    "\n",
    "**(a) [12 points]** Consider the following input 1D array $[w,x,y,z]$. We feed this as input to a linear layer (no activation function) with the following weight matrix and bias vector:\n",
    "\n",
    "$$\n",
    "W\n",
    "= \\begin{bmatrix}\n",
    "    \\frac{1}{2} & 0 & -\\frac{1}{2} & 0 \\\\\n",
    "    0 & \\frac{1}{2} & 0 & -\\frac{1}{2} \\\\\n",
    "    \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\\n",
    "    0 & \\frac{1}{2} & 0 & \\frac{1}{2} \\\\\n",
    "  \\end{bmatrix},\n",
    "\\qquad\n",
    "b\n",
    "= [1, 1, -1, -1].\n",
    "$$\n",
    "\n",
    "What is the output of this linear layer given the input $[w,x,y,z]$? As a reminder, in 95-865, when we take weighted sums, we take the weighted sum of the input with each of the columns of weight matrix $W$ (i.e., the initial weighted sum taken would be of the input $[w,x,y,z]$ with $[\\frac{1}{2},0,\\frac{1}{2},0]$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here with an explanation (do not write code; be sure to provide an explanation--a correct answer without an explanation will not receive full credit):**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**(b)** Consider the following 2-by-2 input image (there's only a single channel, i.e., treat it as grayscale):\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "w & x\\\\\n",
    "y & z\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "We feed this image into a Conv2D layer (no activation function) with the following two kernels:\n",
    "\n",
    "1. The first kernel is:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1/2\\\\\n",
    "1/2\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "with bias value $1$.\n",
    "\n",
    "2. The second kernel is\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "-1/2\\\\\n",
    "1/2\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "with bias value $-1$.\n",
    "\n",
    "**Subpart i. [6 points]** What is the output of the first filter with the bias value added? Your answer should be in terms of $w$, $x$, $y$, and $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here with an explanation (do not write code; be sure to provide an explanation--a correct answer without an explanation will not receive full credit):**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subpart ii. [6 points]** What is the output of the second filter with the bias value added? Your answer should be in terms of $w$, $x$, $y$, and $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here with an explanation (do not write code; be sure to provide an explanation--a correct answer without an explanation will not receive full credit):**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subpart iii. [2 points]** The final output of the Conv2D layer just stacks your answers to subparts i and ii (for simplicity, treat each of the filter outputs as a row, and stack one row after another so that you get two rows). What is this final output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here with an explanation (do not write code; for this subpart it is fine to just state the final answer):**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) [8 points]** How many parameters does the neural net in part **(a)** have? What about the one in part **(b)**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer for number of parameters in the neural net from part (a); please provide a very brief justification:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer for number of parameters in the neural net from part (b); please provide a very brief justification:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) [6 points]** What do you notice about your answer to part **(a)** vs your answer to **(b)-iii**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here (do not write code):**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Predicting Yelp review ratings [60 points]\n",
    "\n",
    "In this problem, we look at Yelp review data, where we evaluate how well two different classifiers are at using a review's text to predict the review's rating (1, 2, 3, 4, or 5 stars). We won't tell you what these classifiers are but we provide you with their outputs for some review data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "Let's first load the Yelp review data. Run the cell below, which populates two Python lists `reviews` and `ratings`. The i-th review in `reviews` has a rating given by the i-th entry in `ratings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# DO NOT MODIFY THIS CELL\n",
    "#\n",
    "import csv\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=5, suppress=True)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "num_header_rows = 1\n",
    "reviews = []  # raw text reviews\n",
    "ratings = []  # these are number of stars from 1 to 5 (always an integer)\n",
    "with open('mystery_data.csv', 'r', encoding='utf-8') as f:\n",
    "    for row in csv.reader(f):\n",
    "        if num_header_rows > 0:\n",
    "            num_header_rows -= 1\n",
    "            continue\n",
    "        else:\n",
    "            reviews.append(row[0])\n",
    "            ratings.append(int(row[1]))\n",
    "ratings = np.array(ratings)\n",
    "\n",
    "num_reviews = len(reviews)\n",
    "\n",
    "print(num_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the two classifier outputs\n",
    "\n",
    "Next, we load the predicted class probabilities for the data above according to mystery models A and B.\n",
    "\n",
    "First, we load in the predicted class probabilities of mystery model A. The i-th row corresponds to the i-th review from `reviews`, and the columns correspond to the probabilities for ratings 1 star, 2 stars, 3 stars, 4 stars, and 5 stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# DO NOT MODIFY THIS CELL\n",
    "#\n",
    "model_A_predicted_probabilities = np.loadtxt('mystery_model_output_A.txt', encoding='utf-8')\n",
    "model_A_predicted_probabilities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# DO NOT MODIFY THIS CELL\n",
    "#\n",
    "model_B_predicted_probabilities = np.loadtxt('mystery_model_output_B.txt', encoding='utf-8')\n",
    "model_B_predicted_probabilities.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Parts **(b)**, **(c)**, and **(d)** build on each other and can be done separately from part **(a)**. Parts **(e)** and **(f)** build on part **(a)**; these can be done separately from parts **(b)**, **(c)**, and **(d)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) [8 points]** Compute the predicted labels for the two different models by filling out the code cell below. When you run it, you should get the raw accuracies of the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# WRITE YOUR CODE HERE\n",
    "#\n",
    "\n",
    "predicted_labels_model_A = np.zeros(len(reviews))  # fill in with predicted labels for the reviews according to model A\n",
    "predicted_labels_model_B = np.zeros(len(reviews))  # fill in with predicted labels for the reviews according to model B\n",
    "\n",
    "#\n",
    "# END OF YOUR CODE\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# do not modify the code below\n",
    "print('Raw accuracy (model A):', np.mean(predicted_labels_model_A == ratings))\n",
    "print('Raw accuracy (model B):', np.mean(predicted_labels_model_B == ratings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) [8 points]** We now convert this problem into a binary classification task, where either 4 or 5 stars is considered the \"positive\" class; meanwhile 1, 2, or 3 stars are grouped together to form the \"negative\" class. We first convert the output of the two mystery models so that the probabilities are for the positive and negative classes.\n",
    "\n",
    "For example, model A's predicted probabilities for the 0-th review are: 0.05 (1 star), 0.05 (2 stars), 0 (3 stars), 0.15 (4 stars), and 0.75 (5 stars). Then the predicted probability for the positive class would be 0.15 + 0.75 (either 4 or 5 stars). The predicted probability for the negative class would be 0.05 + 0.05 + 0 (1, 2, or 3 stars). Note that we do not actually need to store both probabilities since for the 0-th review, the predicted probability for the negative class is just 1 minus the predicted probability of the positive class.\n",
    "\n",
    "Fill in the Python variables below for `model_A_predicted_positive_probability` and `model_B_predicted_positive_probabilities` that store all the predicted probabilities for the positive class according to the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# WRITE YOUR CODE HERE\n",
    "#\n",
    "\n",
    "model_A_predicted_positive_probability = np.zeros(len(reviews))  # i-th entry is the predicted probability of the positive class for the i-th review according to model A\n",
    "model_B_predicted_positive_probability = np.zeros(len(reviews))  # i-th entry is the predicted probability of the positive class for the i-th review according to model B\n",
    "\n",
    "#\n",
    "# END OF YOUR CODE\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# do not modify the code below; these variables are needed in the next part\n",
    "predicted_binary_labels_model_A = 1 * (model_A_predicted_positive_probability >= 0.5)\n",
    "predicted_binary_labels_model_B = 1 * (model_B_predicted_positive_probability >= 0.5)\n",
    "true_binary_labels = 1 * (ratings >= 4)\n",
    "\n",
    "print('Binary raw accuracy (model A):', np.mean(predicted_binary_labels_model_A == true_binary_labels))\n",
    "print('Binary raw accuracy (model B):', np.mean(predicted_binary_labels_model_B == true_binary_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) [16 points]** Fill in the Python function below that calculates the **true positive rate (TPR)**, **false positive rate (FPR)**, **precision**, and **recall** given predicted binary labels `predicted_binary_labels` as well as `true_binary_labels`; each of these are 1D Numpy arrays of 0's and 1's (0 = negative class, 1 = positive class). If you fill in this function correctly, then running the cell below should output the TPR, FPR, precision, and recall for models A and B.\n",
    "\n",
    "**For this part specifically, only use basic Python and Numpy. Do not use any other libraries; do not use sklearn. We want to see that you understand the basic calculations involved.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_TPR_FPR_precision_recall(predicted_binary_labels, true_binary_labels):\n",
    "    # predicted_binary_labels: 1D Numpy array of 0's and 1's; 0 = negative class, 1 = positive class\n",
    "    # true_binary_labels: 1D Numpy array of 0's and 1's; 0 = negative class, 1 = positive class\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # WRITE YOUR CODE HERE\n",
    "    #\n",
    "    \n",
    "    # fill these out with correct values\n",
    "    TPR = -1\n",
    "    FPR = -1\n",
    "    precision = -1\n",
    "    recall = -1\n",
    "    \n",
    "    #\n",
    "    # END OF YOUR CODE\n",
    "    # --------------------------------------------------------------------------\n",
    "    \n",
    "    return TPR, FPR, precision, recall\n",
    "\n",
    "# do not modify the code below\n",
    "TPR_model_A, FPR_model_A, precision_model_A, recall_model_A = \\\n",
    "        calculate_TPR_FPR_precision_recall(predicted_binary_labels_model_A,\n",
    "                                           true_binary_labels)\n",
    "print('TPR (model A):', TPR_model_A)\n",
    "print('FPR (model A):', FPR_model_A)\n",
    "print('Precision (model A):', precision_model_A)\n",
    "print('Recall (model A):', recall_model_A)\n",
    "print()\n",
    "\n",
    "TPR_model_B, FPR_model_B, precision_model_B, recall_model_B = \\\n",
    "        calculate_TPR_FPR_precision_recall(predicted_binary_labels_model_B,\n",
    "                                           true_binary_labels)\n",
    "print('TPR (model B):', TPR_model_B)\n",
    "print('FPR (model B):', FPR_model_B)\n",
    "print('Precision (model B):', precision_model_B)\n",
    "print('Recall (model B):', recall_model_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) [10 points]** Plot the ROC curves for models A and B over each other and compute the areas under their respective ROC curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve  # do not modify\n",
    "plt.figure(figsize=(10,10))  # do not modify\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# WRITE YOUR CODE HERE\n",
    "#\n",
    "\n",
    "AUC_model_A = -1\n",
    "AUC_model_B = -1\n",
    "\n",
    "#\n",
    "# END OF YOUR CODE\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# do not modify the code below\n",
    "print('Area under ROC curve (model A):', AUC_model_A)\n",
    "print('Area under ROC curve (model B):', AUC_model_B)\n",
    "plt.title(\"ROC curve\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.grid()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at the ROC curves you generated, between models A and B, which would you pick for predicting positive (4 or 5 stars) vs negative (1, 2, or 3 stars) classes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here; please very briefly justify your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) [3 points]** One way to diagnose why a classifier might be making errors is to look at where egregious errors might be happening. In this problem, this would correspond to predicting a high rating when the real rating is very low. We have identified one such case for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# DO NOT MODIFY THIS CELL\n",
    "#\n",
    "print(reviews[62])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the real rating for this review, and what is its predicted rating according to models A and B? For this part, please use the rating out of 5 stars and *not* the binary version considered in parts **(b)**, **(c)**, and **(d)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Real rating for review (out of 5 stars):**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicted rating for review according to model A (out of 5 stars):**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicted rating for review according to model B (out of 5 stars):**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f) [15 points]** Let's try to understand why the review in part **(e)** might be misclassified. You find out an extra piece of important information: the underlying classifiers for both models A and B uses a TF-IDF-weighted word counts to represent each review. In fact, the TF-IDF feature vector for this review is given as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# DO NOT MODIFY THIS CELL\n",
    "#\n",
    "tfidf_feature_vector = np.loadtxt('mystery_features.txt', encoding='utf-8')  # TF-IDF feature vector specific to reviews[62]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, the i-th feature value corresponds to the i-th word of the following vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# DO NOT MODIFY THIS CELL\n",
    "#\n",
    "with open('mystery_vocab.txt', 'r', encoding='utf-8') as f:\n",
    "    vocab = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code that lists the top 10 values in `tfidf_feature_vector` along with which specific vocabulary words these top 10 values correspond to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# YOUR CODE HERE\n",
    "#\n",
    "\n",
    "top_10_tfidf_values = []\n",
    "top_10_vocab = []\n",
    "\n",
    "#\n",
    "# END OF YOUR CODE\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# do not modify the code below\n",
    "for rank, (tfidf_value, word) in enumerate(zip(top_10_tfidf_values, top_10_vocab)):\n",
    "    print('Rank %d (TF-IDF weight %f): %s'  % (rank + 1, tfidf_value, word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at the top 10 words with the highest TF-IDF weights, what are some that might be confusing the classifiers (from models A and B), and why do you think these words might confuse such classifiers that represent text using TF-IDF-weighted word counts?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here (briefly explain without any code):**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
