{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 95-865 Spring 2019 Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your name:\n",
    "\n",
    "Your Andrew ID:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This quiz has two problems, a conceptual problem (Problem 1), and a coding problem (Problem 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Conceptual questions [45 points]\n",
    "\n",
    "Parts (a), (b), and (c) can be done in any order. For this problem, your answers should *not* involve any coding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a): True or false, and explain [9 points]\n",
    "\n",
    "For each of the statements below, please specify whether the statement is true or false, *and then briefly explain why*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) When running k-means with 100 random initializations and a fixed choice of the number of clusters k, the best clustering is found using CH index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**: REPLACE THIS WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ii) If we run divisive clustering using k-means (with k=2) to do each split, and we stop after we have 3 clusters, the result is the same as if we fit k-means with 3 clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**: REPLACE THIS WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(iii) Suppose two entities have a high PMI (much larger than 0) and they only co-occur once. This could be because one of the entities is extremely uncommon (i.e., has a low probability of appearing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**: REPLACE THIS WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b): Bigrams [8 points]\n",
    "\n",
    "Consider the following string: \"21100322232\"\n",
    "\n",
    "Treating each character as a separate token, determine the histogram of raw counts of the bigrams in the string above (by hand and not using any code). Remove any bigram term that has the stop word character \"1\" anywhere in it. Your answer should be of the format:\n",
    "\n",
    "```\n",
    "bigram term 1: raw count for bigram term 1\n",
    "bigram term 2: raw count for bigram term 2\n",
    "...\n",
    "```\n",
    "\n",
    "Please present your bigram terms in *increasing* order (rather than decreasing order) of raw count (break ties arbitrarily)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here (no code)**:\n",
    "\n",
    "REPLACE THIS WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c): Isomap [28 points]\n",
    "\n",
    "Consider 5 points A, B, C, D, and E with the following table of Euclidean distances between them (these do *not* account for any sort of nearest neighbor graph structure):\n",
    "\n",
    "| &nbsp; | A          | B          | C          | D          | E          |\n",
    "| ------ | ---------- | ---------- | ---------- | ---------- | ---------- |\n",
    "| A      | 0          | 1          | $\\sqrt{2}$ | $\\sqrt{5}$ | 2          |\n",
    "| B      | 1          | 0          | 1          | 2          | $\\sqrt{5}$ |\n",
    "| C      | $\\sqrt{2}$ | 1          | 0          | 1          | $\\sqrt{2}$ |\n",
    "| D      | $\\sqrt{5}$ | 2          | 1          | 0          | 1          |\n",
    "| E      | 2          | $\\sqrt{5}$ | $\\sqrt{2}$ | 1          | 0          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) What is the single nearest neighbor of point A?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**: REPLACE THIS WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ii) What are the two nearest neighbors of point B?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**: REPLACE THIS WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(iii) What are the two nearest neighbors of point C?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**: REPLACE THIS WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(iv) What are the two nearest neighbors of point D?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**: REPLACE THIS WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(v) What is the single nearest neighbor of point E?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**: REPLACE THIS WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose that we construct a nearest neighbor graph as follows:\n",
    "\n",
    "- We connect point A to its single nearest neighbor.\n",
    "- We connect points B, C, and D each to its two nearest neighbors.\n",
    "- We connect point E to its single nearest neighbor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(vi) What are the edges (\"roads\") that get constructed? Write your answer by listing out the edges that get constructed (you can specify an edge using the phrase \"(A,E)\" to mean an edge between point A and point E; note that edge (A,E) and (E,A) are the same so you do not need to write both of them)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**: REPLACE THIS WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(vii) Now, using the edges you have listed down in the previous step, compute the shortest distances between every pair of points where one can only travel on the edges (\"roads\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Specify your answer by filling out the table below:**\n",
    "\n",
    "| &nbsp; | A          | B          | C          | D          | E          |\n",
    "| ------ | ---------- | ---------- | ---------- | ---------- | ---------- |\n",
    "| A      | ?          | ?          | ?          | ?          | ?          |\n",
    "| B      | ?          | ?          | ?          | ?          | ?          |\n",
    "| C      | ?          | ?          | ?          | ?          | ?          |\n",
    "| D      | ?          | ?          | ?          | ?          | ?          |\n",
    "| E      | ?          | ?          | ?          | ?          | ?          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(viii) What is a valid one-dimensional representation of the points A, B, C, D, and E that would have Euclidean distances (in 1D) exactly match the table you computed in subpart (v), and where the low-dimensional coordinate for A is A' = 0? As a reminder, there is no coding in this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Specify your answer by writing in values for points B', C', D', E' (low-dimensional versions of B, C, D, E):**\n",
    "\n",
    "B' = \n",
    "\n",
    "C' =\n",
    "\n",
    "D' =\n",
    "\n",
    "E' ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ix) Is there more than 1 valid answer to subpart (vi) in which A' = 0? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**: REPLACE THIS WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Analyzing Judge Opinions [55 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, we examine a dataset of majority opinions written by U.S. courts from 1970-2018. While the judges' votes determine the outcomes of each case, the written majority opinions (in this dataset) sets the scope and justification of the precedent that the immediate ruling establishes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell below reads in the data and stores it in two separate lists:\n",
    "\n",
    "- The opinion text (stored in the Python variable `opinions`)\n",
    "- The judgement year (stored in `years`)\n",
    "\n",
    "The opinion texts have already been preprocessed and lemmatized.\n",
    "\n",
    "**Important: Be sure to run the cell below before you run anything else.** *After running the cell below, you can do parts (a) and (b) in either order.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "years = []\n",
    "opinions = []\n",
    "with open('opinions.txt', 'r', encoding='UTF-8') as f:\n",
    "    for line in f:\n",
    "        opinions.append(line.strip())\n",
    "with open('years.txt', 'r', encoding='UTF-8') as f:\n",
    "    for line in f:\n",
    "        years.append(int(line.strip()))\n",
    "years = np.array(years)\n",
    "\n",
    "print(len(opinions)) # should be 4388\n",
    "print(len(years)) # should be 4388"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (a): Extracting Topics with LDA [30 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart 2(a)-i: Construct the TF-IDF matrix [10 points]\n",
    "\n",
    "1. Remove the most common 100 words and store the final vocabulary in a list called `vocabulary` (you should find that there are 106723 words after removing the most common 100). After removing the most common 100 words, make sure to sort the words alphabetically.\n",
    "2. Construct a (4388, 106723) matrix of TF-IDF scores for the opinions using this `vocabulary`. Store this matrix as `tfidf_mat`. (In particular, you should use `TfidfVectorizer` with a single parameter `vocabulary=vocabulary`.)\n",
    "\n",
    "Note: for us, doing the above two steps takes less than 30 seconds to run. If you simply cannot get these to run correctly, you can load in the final values using the code at the beginning of Problem 2(b). We will grade subpart 2(a)-i based on whether you can correctly produce `vocabulary` and `tfidf_mat` *without* using our pre-computed values in Problem 2(b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word.lower() for opinion in opinions for word in opinion.split()] # these are all the words\n",
    "\n",
    "# WARNING: do *not* use spaCy and do *not* remove any words except for the most common 100\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# YOUR CODE HERE\n",
    "#\n",
    "\n",
    "vocabulary = None  # fill in with the correct value; remember to sort these alphabetically\n",
    "tfidf_mat = None  # fill in with the correct value\n",
    "\n",
    "#\n",
    "# END OF YOUR CODE\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart 2(a)-ii: Extract 10 topics using LDA [5 points]\n",
    "\n",
    "We have already fitted an LDA model to the data for you (because this is slow). Using the fitted LDA components learned, print out the top 10 words for each topic. To keep the printing concise, please have your output displayed in the following format:\n",
    "\n",
    "```\n",
    "Topic 0 : list of the top 10 words separated by commas, where the first word is the most probable for the topic\n",
    "Topic 1 : ...\n",
    "Topic 2 : ...\n",
    "...\n",
    "Topic 9 : ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_components = np.loadtxt('lda_components.txt')  # pre-fitted LDA components\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# YOUR CODE HERE\n",
    "#\n",
    "\n",
    "#\n",
    "# END OF YOUR CODE\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart 2(a)-iii: Rank the top 10 words in each topic using \"relevance\" [15 points]\n",
    "\n",
    "\"*Relevance*\" is as an alternate metric to rank words so as to penalize frequent words. This sometimes results in better descriptions of topics than ranking words by the word-topic probabilities as in 2(a)-ii. The relevance of a word $w$ for each topic $k$ is defined as:\n",
    "\n",
    "$$\n",
    "r(k, w) = \\lambda \\textrm{log}(P(w|k)) + (1 - \\lambda)\\textrm{log}(\\frac{P(w|k)}{P(w)})\n",
    "$$\n",
    "\n",
    "Here, $P(w|k)$ is the probability of word $w$ given the topic $k$, and $P(w)$ is the overall probability of the word in the corpus.\n",
    "\n",
    "**Print the top 10 words in each topic ranked by relevance for $\\lambda =$ 0.0, 0.4, 0.8 and 1.0.**\n",
    "\n",
    "Note:\n",
    "   - You will be printing 4 word lists, each one similar to the one you printed in 2(a)-ii.\n",
    "   - When $\\lambda = 1$, the words are ranked by $P(w|k)$ as usual and your results should be identical to 2(a)-ii.\n",
    "   - You must *reuse* the fitted LDA components (do not fit a new LDA model). We are only changing the way the top words of a topic are ranked.\n",
    "\n",
    "*Hint to compute $P(w)$:*\n",
    "\n",
    "To compute $P(w)$, make use of the raw counts you computed in subpart 2(a)-i. Divide by the total sum of raw counts across words that didn't get removed.\n",
    "\n",
    "*Hints to compute $P(w|k)$:*\n",
    "\n",
    "You already have a matrix containing the *unnormalized* $P(w|k)$; this needs to be normalized by making sure each row sums to one.\n",
    "\n",
    "**Important:** It's okay if you find that the topics are not very interpretable. We will be grading based on whether you can do the calculations mentioned above correctly rather than whether the topics are interpretable. In part (b), we will be looking at an approach that results in more interpretable topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# YOUR CODE HERE (to compute P(w))\n",
    "#\n",
    "\n",
    "P_w = np.zeros(len(vocabulary)) # fill/replace this 106723-length array with P(w)\n",
    "\n",
    "#\n",
    "# END OF YOUR CODE\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# YOUR CODE HERE (to compute P(w|k))\n",
    "#\n",
    "\n",
    "P_wk = np.zeros((10, len(vocabulary))) # replace this 10 x 106723 matrix with P(w|k)\n",
    "\n",
    "#\n",
    "# END OF YOUR CODE\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "for lam in [0, 0.4, 0.8, 1.0]:\n",
    "    print('[lambda = ', lam, ']', sep='')\n",
    "    \n",
    "    # Print the top 10 words by reusing your code in 2(a)-ii\n",
    "    # The only change required is to sort by relevance, instead of\n",
    "    # unnormalized word probabilities in each topic\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    # YOUR CODE HERE\n",
    "    #\n",
    "    \n",
    "    # 1. compute relevance scores\n",
    "    \n",
    "    # 2. print the top 10 words per topic sorted by relevance\n",
    "    \n",
    "    #\n",
    "    # END OF YOUR CODE\n",
    "    # --------------------------------------------------------------------------\n",
    "    \n",
    "    print()  # empty new line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b): Extracting Topics with LSA and Clustering [25 points]\n",
    "\n",
    "Latent Semantic Analysis (LSA) is a dimensionality reduction method similar to PCA. For the purposes of this problem, you do not need to know what LSA does. We have already reduced the dimensionality of the dataset using LSA. Now, as an alternative to LDA, we extract topics by clustering this low-dimensional LSA representation of the data. *In particular, we will interpret the resulting clusters to be the topics.*\n",
    "\n",
    "The code below loads the LSA low-dimensional representation of the original data into the variable `lsa_mat`. The code then also loads the TF-IDF matrix of the documents as `tfidf_mat`, along with the vocabulary as `vocabulary2`. If you correctly constructed these in 2(a)-i, they should be identical. **Be sure to run the cell below before proceeding to subsequent subparts.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "lsa_mat = np.loadtxt(\"lsa_mat.csv\", delimiter=\",\")   # LSA low-dimensional representation of the data\n",
    "print(lsa_mat.shape) # should be (4388, 300)\n",
    "\n",
    "# the code below loads precomputed versions of the correct output to subpart 2(a)-i\n",
    "import scipy.sparse\n",
    "tfidf_mat = scipy.sparse.load_npz('tfidf.npz')\n",
    "vocabulary2 = [word.strip() for word in open('vocabulary.txt', 'r', encoding='UTF-8').readlines()]\n",
    "print(tfidf_mat.shape)\n",
    "print(len(vocabulary2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart 2(b)-i: Cluster the LSA matrix using k-means [5 points]\n",
    "\n",
    "Set the number of clusters to 10 and `random_state=11`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# YOUR CODE HERE\n",
    "#\n",
    "\n",
    "cluster_assignments = None  # fill this out correctly\n",
    "\n",
    "#\n",
    "# END OF YOUR CODE\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart 2(b)-ii: Construct \"topics\" using the cluster labels [10 points]\n",
    "\n",
    "Remember how in lecture, for each cluster, we computed the average feature vector for that cluster? We now build on this idea.\n",
    "\n",
    "For each cluster $k$:\n",
    "\n",
    "   - Compute the average TF-IDF row (this is like the average feature vector) for all documents inside cluster $k$: call this array `mean_tf_idf_vector`. (Recall that the feature vectors in our current setup is stored in `tfidf_mat`; we have converted this into a 2D numpy array `tfidf_mat_as_numpy_array` for you.)\n",
    "   - Print out the top 10 words (using `vocabulary2`) for the \"topic\" of cluster $k$ by sorting `mean_tf_idf_vector` in decreasing order.\n",
    "\n",
    "Your final printed output should be of the following format:\n",
    "\n",
    "```\n",
    "Topic 0 : list of the top 10 words separated by commas, where the first word is the most probable for the topic\n",
    "Topic 1 : ...\n",
    "Topic 2 : ...\n",
    "...\n",
    "Topic 9 : ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_mat_as_numpy_array = np.asarray(tfidf_mat.todense())\n",
    "\n",
    "top_words_lsa = {}  # this will be helpful for subpart 2(b)-iii\n",
    "for i in range(10):\n",
    "    # -------------------------------------------------------------------------\n",
    "    # YOUR CODE HERE\n",
    "    #\n",
    "    mean_tf_idf_vector = None  # fill this out correctly making use of `tfidf_mat_as_numpy_array`\n",
    "    top_words = None  # fill this out correctly\n",
    "    top_words_lsa[i] = top_words  # DO NOT MODIFY THIS LINE\n",
    "    \n",
    "    # add your printing code here\n",
    "    \n",
    "    #\n",
    "    # END OF YOUR CODE\n",
    "    # -------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart 2(b)-iii: Plot the prevalence of the LSA topics over time [10 points]\n",
    "\n",
    "Each LSA cluster corresponds to a topic.\n",
    "\n",
    "   1. Use the `years` array to plot how frequently each topic appears over time. In particular, for this plot, please let the frequency of a topic in a year be the *fraction* of documents having that topic in that year. There should be one line for each topic. *Also, make sure that in your plot, you have the years sorted chronologically. Each year should of course only appear once in the plot.*\n",
    "   2. Label the line for each topic with its 3 top words (using the topic-words dictionary `top_words_lsa` in subpart 2(b)-ii. Make sure you plot the legend to display these labels. (We suggest using `plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# YOUR CODE HERE\n",
    "#\n",
    "\n",
    "#\n",
    "# END OF YOUR CODE\n",
    "# -----------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
