{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 95-865 Fall 2019 Quiz 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your name:**\n",
    "\n",
    "**Your Andrew ID:**\n",
    "\n",
    "**Your section (A2/B2/K2):**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the quiz, please run this next cell first. **Important:** The exam is written so that all the packages that need to be imported are already imported in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# DO NOT MODIFY THIS CELL\n",
    "#\n",
    "%matplotlib inline\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 [45 points total]\n",
    "\n",
    "This problem tests concepts from basic text processing including frequency and co-occurrence analysis. Consider the following Python string that we shall analyze (be sure to run the next cell):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# DO NOT MODIFY THIS CELL\n",
    "#\n",
    "text = 'm m h f d h l j j g f h i l o t p q s u x t t x x s w v t o m m h e d i e d d d h c b b b a c e h f c h e a a e f f g j i e e j o t w s o q u y w x z x y v q o o s x w v w v u s r q t s q v x z z z z u q q u s n n i e c e c a f d c c f a f j e c d h e j g e d c g e e b c d b e g j g e a a a d d i l o l j g b a a d i n k n m k f e d g k n i l l p k o p p n j m h g k l l n q t x u x y w x z z v r s v y w t r w x v w w y w r u t y z x x x w z w u y y w v v t r t x t x z x y v s q u z x x z z z v s p s o o r q l i i i d i e b a d i l k j e j h j h e b b c b f e c f f f c h h j l m i j l i g c b f f j n k f j f j o j k f k j m l j h k n p k i l n p u q t s u p o s n s t s p o p n s q s v v q t t s u z w v r p n r o o l j j o q n p l m r r m h m k p l q u y z u x z u x y t x s q r n p o s p k n k m p o n j l m q p l l p r n l l n l m n p t p t u p n q p l k k f d b e e e a a c f i h i m k i f b c c b d d e b b a a c e d f g e c b a e b e e a a f d f f b f i j i k i i g h g i g b f j h j j j'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we tokenize by splitting on spaces, so the vocabulary just consists of each letter of the English alphabet. There is no lemmatization. You can assume all letters are lowercase. Do not use stop words. Write your code using only basic Python, matplotlib, and, if you would like, NumPy. Do not use Pandas or any other external libraries.\n",
    "\n",
    "**(a) [5 points]** Plot a histogram showing how often each term (i.e., English letter) appears, where the x-axis has the letters sorted in alphabetical order, and the y-axis **has frequencies as fractions and not raw counts** (so if we add up the frequencies for all the letters, we get 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# YOUR CODE GOES HERE\n",
    "#\n",
    "\n",
    "#\n",
    "# END OF YOUR CODE\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) [15 points]** Compute the co-occurrence table between different terms in the following manner: \n",
    "\n",
    "1. Initialize the co-occurrence between every pair of English alphabet letters to be 0.\n",
    "2. Look at every consecutive pair of terms in the text (so the first pair is `'m'` & `'m'`, the second is `'m'` & `'h'`, etc); for each pair we encounter, add 1 to the number of times we see that pair appear (disregarding order, so `m` & `h` co-occurring is the same as `h` and `m` co-occurring). _In particular, this means that we are only counting co-occurrences for terms that appear right next to each other in the text._\n",
    "\n",
    "After computing the table, please display it as a heat map (using matplotlib's `imshow`), where rows and columns correspond to English alphabet letters sorted alphabetically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# YOUR CODE GOES HERE\n",
    "#\n",
    "\n",
    "#\n",
    "# END OF YOUR CODE\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) [15 points]** Compute what the co-occurrence table would look like if adjacent terms were independent. To do this, make use of the marginal probabilities for terms that you determined in part (a).\n",
    "\n",
    "After computing the table, please display it as a heat map (using matplotlib's `imshow`), where rows and columns correspond to English alphabet letters sorted alphabetically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# YOUR CODE GOES HERE\n",
    "#\n",
    "\n",
    "#\n",
    "# END OF YOUR CODE\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** Only answer this part if you've already completed parts (b) and (c).\n",
    "\n",
    "**Subpart i. [5 points]** What is the PMI for `'a'` and `'z'`? Be sure to briefly explain your answer. (Note: As with the previous two parts, we are considering terms that are adjacent to each other in the text when computing raw co-occurrences.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here (do not write code in your answer):** REPLACE THIS TEXT WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subpart ii. [5 points]** Is it true that each alphabet letter in this sequence only co-occurs with another letter that is sufficiently close in terms of alphabetical ordering? If so, how close? Be sure to briefly explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here (do not write code in your answer):** REPLACE THIS TEXT WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 [55 points]\n",
    "\n",
    "In this problem, we explore some Yelp review data. We are interested in understanding what might explain the rating (out of 5 stars) of a review based on the review's text.\n",
    "\n",
    "Let's first load in the data in by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_header_rows = 1\n",
    "reviews = []  # raw text reviews\n",
    "ratings = []  # these are number of stars from 1 to 5 (always an integer)\n",
    "with open('yelp_review_10k_text_and_star.csv', 'r') as f:\n",
    "    for row in csv.reader(f):\n",
    "        if num_header_rows > 0:\n",
    "            num_header_rows -= 1\n",
    "            continue\n",
    "        else:\n",
    "            reviews.append(row[1])\n",
    "            ratings.append(int(row[0]))\n",
    "ratings = np.array(ratings)\n",
    "\n",
    "num_reviews = len(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** A simple question one might ask is: _how does review length relate to the rating?_ Let's try to answer this question by looking at the data.\n",
    "\n",
    "**Subpart i. [4 points]** For each of the ratings (1, 2, 3, 4, and 5), print out the average review length (length of string, i.e., number of characters in the string) and the standard deviation. Your output (from the `print` function) should be formatted like this:\n",
    "\n",
    "Rating: 1, average review length: *answer*, std dev: *answer*<br>\n",
    "Rating: 2, average review length: *answer*, std dev: *answer*<br>\n",
    "Rating: 3, average review length: *answer*, std dev: *answer*<br>\n",
    "Rating: 4, average review length: *answer*, std dev: *answer*<br>\n",
    "Rating: 5, average review length: *answer*, std dev: *answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# YOUR CODE GOES HERE\n",
    "#\n",
    "\n",
    "#\n",
    "# END OF YOUR CODE\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subpart ii. [4 points]** Building off your solution to subpart i, plot a line chart that shows average review length (along the y-axis) vs the number of stars (along the x-axis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# YOUR CODE GOES HERE\n",
    "#\n",
    "\n",
    "#\n",
    "# END OF YOUR CODE\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subpart iii. [2 points]** Based on your answer to the previous subparts, what general trend do you observe regarding the average review length vs the number of stars?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here (do not write code in your answer):** REPLACE THIS TEXT WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Using only the length of reviews does not actually use any semantic information from the text. Let's now try to take advantage of text.\n",
    "\n",
    "**Subpart i. [5 points]** We have provided a preprocessing function below. Apply this function to each review to obtain a list of preprocessed reviews (without modifying the original `reviews` variable). After computing this list, print out the 0-th preprocessed review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# DO NOT MODIFY THIS CELL\n",
    "#\n",
    "def preprocess_review(review):\n",
    "    output = review\n",
    "    for c in string.punctuation:\n",
    "        output = output.replace(c, '')\n",
    "    output = re.sub(r'\\S*\\d\\S*', '', output)\n",
    "    output = re.sub(r'[^\\w\\s]', '', output)\n",
    "    output = output.lower()\n",
    "    output = ' '.join([piece.strip() for piece in output.split()])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# YOUR CODE GOES HERE\n",
    "#\n",
    "\n",
    "processed_reviews = ['incorrect answer']  # for you to fill out; do not change this variable name\n",
    "\n",
    "#\n",
    "# END OF YOUR CODE\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "print(processed_reviews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subpart ii. [5 points]** Use `TfidfVectorizer` (note that this has already been imported for you; don't import it again) to transform the data into TF-IDF-weighted word counts; for `TfidfVectorizer`, use the parameters `stop_words=\"english\"`, `min_df=500`, and `max_df=0.8` (do not specify any other parameters). *We will apply PCA to these TF-IDF-weighted word counts.*\n",
    "\n",
    "After computing the TF-IDF-weighted word counts per review, plot what fraction of the total variance is explained by PCA when projecting the TF-IDF-weighted vectors to 10, 20, 30, 40, 50, 60, 70, 80, 90, and 100 dimensions (along the y-axis should be variance explained, and along the x-axis should be the number of PCA dimensions).\n",
    "\n",
    "*Hint:* There are multiple ways to write the code here to produce the same answer. As a reminder, `TfidfVectorizer` has a `transform` function that outputs a sparse matrix, which--depending on your solution--you may have to convert to a 2D NumPy array. This conversion can be done via the `toarray()` function of a sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# YOUR CODE GOES HERE\n",
    "#\n",
    "\n",
    "# 1. Compute TF-IDF-weighted word counts and store into `X_tfidf`; do not change\n",
    "# this variable name since it's used in a later problem subpart!!!\n",
    "\n",
    "X_tfidf = None  # for you to fill out; do not change this variable name\n",
    "\n",
    "# 2. Plot variance explained vs number of PCA components\n",
    "\n",
    "\n",
    "#\n",
    "# END OF YOUR CODE\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximately how many PCA components/dimensions are needed to explain 90% of the variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here (do not write code in your answer):** REPLACE THIS TEXT WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subpart iii. [10 points]** Use PCA to visualize the data in 2 dimensions.\n",
    "\n",
    "We have provide some plotting code for you, with a slight twist: the plotting code asks that you identify the PCA low-dimensional representations specific to reviews with a particular rating (e.g., extracting low-dimneionsal representations only for reviews with rating 1, and then only for reviews with rating 2, etc). Please fill in this code. (The reason we have this slight twist compared to what we did in lecture demos is to get a legend to show up that automatically labels which points correspond to which ratings.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# YOUR CODE GOES HERE\n",
    "#\n",
    "\n",
    "# Fit a PCA model with 2 components\n",
    "\n",
    "#\n",
    "# END OF YOUR CODE\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN THIS CELL, ONLY WRITE CODE WITHIN THE BLOCK THAT SAYS \"YOUR CODE GOES HERE\"\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for rating in range(1, 6):\n",
    "    # --------------------------------------------------------------------------\n",
    "    # YOUR CODE GOES HERE\n",
    "    #\n",
    "    \n",
    "    # Find all PCA low-dimensional representations corresponding to `rating`\n",
    "    # and store their first and second principal component values in the\n",
    "    # variables below (i.e., the 0-th and 1-st PCA axes if we index from 0)\n",
    "    pca_axis0 = []  # for you to fill out\n",
    "    pca_axis1 = []  # for you to fill out\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    ax.scatter(pca_axis0, pca_axis1,\n",
    "               c='C%d' % (rating - 1), label=rating,\n",
    "               cmap='spectral', alpha=0.4)\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is PCA a good choice for visualizing the TF-IDF-weighted word count vectors for the Yelp review data? Briefly explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here (do not write code in your answer):** REPLACE THIS TEXT WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Building off of part (b), it turns out that t-SNE struggles with the TF-IDF-weighted word counts. We will spare you the pain and agony of trying this yourself, especially as t-SNE is slow to run.\n",
    "\n",
    "Rather than using TF-IDF-weighting, we have gone ahead and done something for you: we create an alternative 100-dimensional representation for the data using some method we haven't discussed thus far in class. You can load in this representation via running the following cell (the i-th row still corresponds to the i-th review):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# DO NOT MODIFY THIS CELL\n",
    "#\n",
    "X_mystery = np.loadtxt('mystery.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subpart i. [5 points]** Using this new representation of the data instead of the TF-IDF-weighted word counts, what is the total amount of variance explained using 2 PCA components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# YOUR CODE GOES HERE\n",
    "#\n",
    "\n",
    "# 1. Fit a PCA model with 2 components\n",
    "\n",
    "# 2. Print out the model's total explained variance (the sum)\n",
    "\n",
    "#\n",
    "# END OF YOUR CODE\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subpart ii. [5 points]** Repeat part (b)-iii using the new representation instead, i.e., plot the 2D PCA plot with a legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# YOUR CODE GOES HERE\n",
    "#\n",
    "\n",
    "#\n",
    "# END OF YOUR CODE\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subpart iii. [15 points]** PCA and t-SNE both use Euclidean distance to measure how far apart input points are. Below, we have computed the Euclidean distance tables between every pair of points using TF-IDF-weighted word count vectors as well as the mystery representation we've precomputed for you. In particular, each distance table is a 2D NumPy array where the i-th row, j-th column tells you how close the i-th review is to the j-th review.\n",
    "\n",
    "Let's first load in these distance tables by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# DO NOT MODIFY THIS CELL\n",
    "#\n",
    "distance_table_tfidf = euclidean_distances(X_tfidf)\n",
    "distance_table_mystery = euclidean_distances(X_mystery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lecture, we talked about how to debug how good a distance or similarity function is by comparing a data point with its closest other data points (using the user-specified distance or similarity function), and seeing if the closest other data points qualitatively make sense.\n",
    "\n",
    "Fill in the Python function below that, given a distance table `distance_table` and a review index `review_idx`, finds the `k` closest *other* reviews (do not include `review_idx` in these `k` other reviews) and computes the average rating of these `k` closest reviews. Recall that you have the rating for each review (stored in the Python variable `ratings`).\n",
    "\n",
    "We have already provided code that prints out (if the `verbose` flag is set to `True`) the  input review, the reviews for each of the `k` closest other reviews that your code finds the indices for, the rating for the input review, and the average rating that you compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN THIS CELL, ONLY WRITE CODE WITHIN THE BLOCK THAT SAYS \"YOUR CODE GOES HERE\"\n",
    "\n",
    "def find_closest_reviews(distance_table, review_idx, k=10, verbose=True):\n",
    "    if verbose:\n",
    "        print('[Input review that we find the', k, 'closest reviews of]')\n",
    "        print(processed_reviews[review_idx])\n",
    "        print()\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    # YOUR CODE GOES HERE\n",
    "    #\n",
    "    \n",
    "    # Find the `k` closest points to the review given by `review_idx`, where how\n",
    "    # close points are is given by `distance_table`\n",
    "    \n",
    "    indices_for_closest_k_reviews = []  # for you to fill out\n",
    "    average_rating_of_closest_k_reviews = 0  # for you to fill out\n",
    "    \n",
    "    #\n",
    "    # END OF YOUR CODE\n",
    "    # --------------------------------------------------------------------------\n",
    "    \n",
    "    if verbose:\n",
    "        for idx in range(k):\n",
    "            print('[%d-closest review]' % (idx + 1))\n",
    "            print(processed_reviews[indices_for_closest_k_reviews[idx]])\n",
    "            print()\n",
    "        print('Input review rating:', ratings[review_idx])\n",
    "        print('Average rating of', k, 'closest reviews:',\n",
    "              average_rating_of_closest_k_reviews)\n",
    "    return ratings[review_idx], average_rating_of_closest_k_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After writing the above function, run the following two cells. If your code is working correctly, these should give a clear example of where the TF-IDF representation yields a particularly bad choice of closest 10 data points to an input review compared to the mystery representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# DO NOT MODIFY THIS CELL\n",
    "#\n",
    "find_closest_reviews(distance_table_tfidf, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# DO NOT MODIFY THIS CELL\n",
    "#\n",
    "find_closest_reviews(distance_table_mystery, 200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
