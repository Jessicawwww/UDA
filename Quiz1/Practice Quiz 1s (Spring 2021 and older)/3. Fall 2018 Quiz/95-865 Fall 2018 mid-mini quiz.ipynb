{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 95-865 Fall 2018 Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your name:\n",
    "\n",
    "Your Andrew ID:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two problems. The first problem is mainly about making sure you can code up various things from the first half of 95-865. The second problem is more conceptual: we give you intermediate outputs of some analyses that we have done on a particular dataset, and ask you to answer some follow-up questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS BLOCK (BUT YOU SHOULD RUN THIS FIRST)\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 [50 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) [5 points]** Begin by reading in the book titles from the file `all_book_titles.txt` (which should be in the same folder as this Jupyter notebook). Complete the following tasks:\n",
    "\n",
    "   1. Read the book titles into a list of strings named `titles`.\n",
    "   2. Display the first 8 titles in your list. \n",
    "   3. Print the total number of book titles in your list.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) [25 points]** Build the frequency table. Specifically, complete the following tasks:\n",
    "\n",
    "  1. Process the text by separating and lemmatizing the words.\n",
    "  2. Count the number of times each word appears in the file and build a frequency table. \n",
    "  3. Sort the table and print the top 20 most frequent words, along with their frequencies and ranks.\n",
    "  4. From results in step 3, which of those 20 words that you think could be added into stopwords for this problem? Add those new stopwords you chose, and recompute the frequency table. Plot the top 10 words in bar plots.\n",
    "  \n",
    "  \n",
    "Notes: \n",
    "* When counting the words, use raw counts as the \"frequency\". DO NOT divide by the total number of words in the file. \n",
    "* DO NOT include words (tokens) that are stopwords/punctuation/spaces/short words with length<3. \n",
    "* For stopwords, use the stopwords provided in the file `stopwords.txt` (which should be in the same folder as this Jupyter notebook).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) [20 points]** We now use k-means to cluster the book titles. The goal of this problem is to (1) choose the best `k` based on the silhouette score and (2) interpret the clustering results.\n",
    "\n",
    "\n",
    "* (1) For each k=10,30,50,...,190: cluster all the book titles (in full-dimensionality, so don't reduce dimensions!) into k clusters, and compute and print the silhouette score for this choice of k. (Note: you don't need to know the details of what the [\"silhouette score\"](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score) is computing!--for the purposes of this exam, you just need to know that a higher silhouette score is better.)\n",
    "\n",
    "* (2) After you obtain the best `k`, print out the book titles in the first 5 clusters given by the best model, and interpret each cluster briefly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we cluster the book titles using k-means, we need to represent each title as a feature vector, very similar to building a histogram. We have provided this code for you. The code uses scikit-learn's `TfidfVectorizer` class, which represents each document as a feature vector. The resulting variable `X` is a 2-D numpy array, and has rows corresponding to book titles, and columns corresponding to different words. For the purposes of this quiz, you do *not* have to worry about how this works. Just treat `X` as a 2D numpy array where rows are feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY CODE IN THIS CELL\n",
    "\n",
    "# If your code above works correctly, then running this cell should result\n",
    "# in the variable `X` being a 2D numpy array with shape (2373, 291)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=10, max_df=0.8)\n",
    "X = vectorizer.fit_transform(titles)  # This is stored as a sparse array, so as to save memory\n",
    "\n",
    "print(X.shape) # The output should be (2373, 291)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please write down your interpretation of the clusters**: WRITE YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 [50 points]\n",
    "\n",
    "In this problem, we examine part of the Fashion MNIST dataset by Zalando Research. In particular, we look at 10,000 images, each 28 pixels by 28 pixels, where each image is of one of the following 10 items:\n",
    "\n",
    "- T-shirt/top (encoded by the integer 0)\n",
    "- Trouser (encoded by the integer 1)\n",
    "- Pullover (2)\n",
    "- Dress (3)\n",
    "- Coat (4)\n",
    "- Sandal (5)\n",
    "- Shirt (6)\n",
    "- Sneaker (7)\n",
    "- Bag (8)\n",
    "- Ankle boot (9)\n",
    "\n",
    "We are intentionally *not* giving you the raw images. Instead, we provide you with some intermediate results from some analyses we have already done, and we ask you some follow-up questions.\n",
    "\n",
    "In what follows, assume the following: each image is represented as a 784-dimensional feature vector (corresponding to flattening the image's raw grayscale pixels to go from an 28 by 28 image). For each feature vector, each feature is stored as a value between 0 (black) and 1 (white). Below is a plot of many example images from the Fashion MNIST dataset:\n",
    "\n",
    "![Fashion MNIST examples](./fashion-mnist-sprite.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) [5 points]** We ran PCA and got the following plot:\n",
    "\n",
    "![PCA plot](./Fall2018_quiz_PCA.png)\n",
    "\n",
    "Note that the horizontal axis corresponds to the first principal component, and the vertical axis corresponds to the second principal component. The explained variance ratio for the first and second principal components are 0.2902809 and 0.17902619, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the explained variance ratios are quite low, your friend Alice says that actually PCA is not too bad if you're mainly concerned about distinguishing between images that are of sneakers/sandals vs those that are not, and moreover you don't care about trying to tell apart sneakers from sandals. In fact, she says that even just using PCA with 1 dimension is fine. Why might Alice say this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here (no code is needed for this answer):** WRITE YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) [10 points]** We have provided the weights for the first two PCA components below, along with the feature vectors of 10 images. We have also provided a mean offset vector. Compute the low-dimensional PCA representation of these 10 images *using the weights provided*, where as a preprocessing step, *be sure to subtract the mean offset vector from each feature vector*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "\n",
    "weights_for_first_principal_component = np.loadtxt('Fall2018_quiz_PCA_weights_1.txt')\n",
    "weights_for_second_principal_component = np.loadtxt('Fall2018_quiz_PCA_weights_2.txt')\n",
    "mean_offset_vector = np.loadtxt('Fall2018_quiz_PCA_mean_offset.txt')\n",
    "ten_feature_vectors = np.loadtxt('Fall2018_quiz_PCA_ten_feature_vectors.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_dimensional_vectors = np.zeros((10, 2))  # for you to fill out\n",
    "\n",
    "# YOUR CODE GOES HERE THAT COMPUTES THE LOW DIMENSIONAL REPRESENTATION OF THE TEN FEATURE VECTORS\n",
    "\n",
    "print(low_dimensional_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) [25 points]** We ran t-SNE and ended up with the following plot:\n",
    "\n",
    "![t-SNE image](./Fall2018_quiz_tSNE.png)\n",
    "\n",
    "Below, we provide a *subsample* of the images used to construct the above t-SNE plot, along with their low-dimensional t-SNE representations and their labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_vectors = np.loadtxt('Fall2018_quiz_subsample_image_vectors.txt')\n",
    "tsne_vectors = np.loadtxt('Fall2018_quiz_subsample_tsne_vectors.txt')\n",
    "labels = np.loadtxt('Fall2018_quiz_subsample_labels.txt').astype(np.int)\n",
    "\n",
    "# the i-th row of `image_vectors` has t-SNE representation given by `tsne_vectors[i]` and has label `labels[i]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at the t-SNE plot, your friend Bob wants to know why coats and pullovers overlap so much in the plot. He wonders whether the two types of clothing are just really hard to tell apart. You decide to help Bob answer this question.\n",
    "\n",
    "Using the above subset of images (in the previous cell), write code that completes the following tasks:\n",
    "\n",
    "1. Compute the mean 2D coordinate of the low-dimensional t-SNE representations of images corresponding to coats (using the known labels)\n",
    "2. Find the coat image with t-SNE representation closest (in Euclidean distance) to the mean computed in step 1; plot this coat image as a 28-by-28 pixel image using the `plt.imshow` function with attribute `cmap='gray'`\n",
    "3. Find the pullover image with t-SNE representation closest (in Euclidean distance) to the mean computed in step 1; plot this pullover image again as a 28-by-28 pixel image using `plt.imshow` with `cmap='gray'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice from your images for steps 2 and 3 above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here (no code is needed for this answer)**: WRITE YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) [10 points]** In class, we talked about a simple standardization technique in which we subtract off the mean of each feature value and also divide by the standard deviation. Your friend Alice says that since the images we are looking at consist of clothing articles and fashion accessories with the same backdrop in the image, dividing each feature by the standard deviation is a bad idea for preprocessing in this setting. Is she right? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here (no code is needed for this answer)**: WRITE YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
