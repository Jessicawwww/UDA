{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2ca16d4",
   "metadata": {
    "id": "5cc8a4b0"
   },
   "source": [
    "# CMU 95-865 - Unstructured Data Analytics Fall 2021 Quiz 1\n",
    "\n",
    "This is an 80 minute exam. We will only grade what is submitted via Canvas.\n",
    "\n",
    "You must fill in your name and your Andrew ID for this quiz to be graded. Moreover, filling out your name and Andrew ID below will serve as your agreement with us, the course staff, that you did not collaborate with anyone on this exam and that what you submit is truly your own individual work and not that of anyone else. Violations found will result in severe penalties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c915895",
   "metadata": {
    "id": "4084baf4"
   },
   "source": [
    "Your name: solutions\n",
    "\n",
    "Your Andrew ID: n/a\n",
    "\n",
    "**Warning: If you leave the above blank, your quiz will not be graded.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa94da8",
   "metadata": {
    "id": "18af11b8"
   },
   "source": [
    "**Important:** There are 3 problems that can be done in any order. The very first part of Problem 1 and most of Problem 3 involve coding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2352e0e",
   "metadata": {
    "id": "a88b1083"
   },
   "source": [
    "## Problem 1: Drug Consumption, Revisited (1 very brief coding part, and 3 non-coding parts) [20 points]\n",
    "\n",
    "Remember how in the clustering lectures, we analyzed the [UCI drug consumption dataset](https://archive.ics.uci.edu/ml/datasets/Drug+consumption+%28quantified%29)? We now revisit that dataset, except looking only at the features that correspond to what drugs people took. We specifically want to better understand patterns in the usage of different pairs of drugs. For instance, people who use one drug (such as amphetamines) might be more likely to use another drug (such as cannabis).\n",
    "\n",
    "There are a total of 18 real drugs that the study tracked, and also 1 fake drug (called Semeron) that the study included (basically it was added as a sanity check to see if some people just over-claimed what drugs they took, even claiming to take a fictituous drug). Thus, there are 19 drugs that we keep track of. For each respondent in the dataset, we keep track of whether they took each of the drugs or not (so that each feature is binary). Note that in the original dataset, they actually also keep track of when was the last time each respondent took each drug, if they claimed to have taken the drug--we will *not* use this information and will only be looking at whether they ever took the drug or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dd8f65",
   "metadata": {
    "id": "73722f15"
   },
   "source": [
    "**(a)  [5 points] Calculating phi-squared values.** As a warm-up to the later questions, please compute the phi-squared values for each of the following two co-occurrence tables (these are based on the real data!):\n",
    "\n",
    "```\n",
    "+---------------+-------------------+------------------+\n",
    "|               |   chocolate : yes |   chocolate : no |\n",
    "+===============+===================+==================+\n",
    "| alcohol : yes |              1822 |               29 |\n",
    "+---------------+-------------------+------------------+\n",
    "| alcohol : no  |                31 |                3 |\n",
    "+---------------+-------------------+------------------+\n",
    "\n",
    "+---------------+----------------------------------+---------------------------------+\n",
    "|               |   volatile substance abuse : yes |   volatile substance abuse : no |\n",
    "+===============+==================================+=================================+\n",
    "| ecstasy : yes |                              317 |                             547 |\n",
    "+---------------+----------------------------------+---------------------------------+\n",
    "| ecstasy : no  |                              113 |                             908 |\n",
    "+---------------+----------------------------------+---------------------------------+\n",
    "```\n",
    "\n",
    "Note that the numbers in the table correspond to the number of participants who fall into that particular box (e.g., 29 respondents have had alcohol but never had chocolate in their lives)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6018bbb",
   "metadata": {
    "id": "de2f600c"
   },
   "source": [
    "**Phi-squared value for alcohol & chocolate**: 0.005589221098449516"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26caa256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005589221098449516\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "co_occur_table = np.array([[1822, 29], [31, 3]])\n",
    "joint_prob_table = co_occur_table / co_occur_table.sum()\n",
    "joint_prob_table_indep = np.outer(joint_prob_table.sum(axis=1), joint_prob_table.sum(axis=0))\n",
    "phi_squared = ((joint_prob_table - joint_prob_table_indep)**2 / joint_prob_table_indep).sum()\n",
    "print(phi_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201ec38e",
   "metadata": {
    "id": "4353e395"
   },
   "source": [
    "**Phi-squared value for ectasy & volatile substance abuse (also please show your work):** 0.09256398054008015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e93b122d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09256398054008015\n"
     ]
    }
   ],
   "source": [
    "co_occur_table = np.array([[317, 547], [113, 908]])\n",
    "joint_prob_table = co_occur_table / co_occur_table.sum()\n",
    "joint_prob_table_indep = np.outer(joint_prob_table.sum(axis=1), joint_prob_table.sum(axis=0))\n",
    "phi_squared = ((joint_prob_table - joint_prob_table_indep)**2 / joint_prob_table_indep).sum()\n",
    "print(phi_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe4385b",
   "metadata": {
    "id": "202660e3"
   },
   "source": [
    "---\n",
    "\n",
    "Of course, there aren't just 2 tables. Since there are 19 drugs, there are a total of $\\frac{19\\times18}{2} = 171$ co-occurrence tables (each of these are 2-by-2) to consider! We aren't going to ask you to compute the phi-squared values of all of these by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcce7e43",
   "metadata": {
    "id": "ae734242"
   },
   "source": [
    "**(b) [5 points] Max phi-squared value.** What is the maximum possible phi-squared value for 2-by-2 co-occurrence tables like the ones above (where each row and each column has a nonzero sum)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacd5f69",
   "metadata": {
    "id": "cca74913"
   },
   "source": [
    "**Your answer here (please be sure to include an explanation of how you got your answer):** min(2, 2) - 1 = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9f485a",
   "metadata": {
    "id": "70136b34"
   },
   "source": [
    "**(c) [5 points] Ranking pairs of drugs.** True or false: to rank the 171 drug pairs, we cannot use phi-squared or Chi-squared, and instead, we have to use Cramer's V."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2167f7",
   "metadata": {
    "id": "bdd4bc05"
   },
   "source": [
    "**Your answer here (please indicate true or false, and regardless of which option you put, you must explain your answer to receive any credit):** False; in this case, we can in fact use phi-squared since all the tables are the same size, so the phi-squared values are all of the same scale (between 0 and 1). We can also use Chi-squared since the total number of co-occurrences is always the same across the different drug pairs (and is equal to the number of respondents: 1885)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190da8c9",
   "metadata": {
    "id": "47d8cde1"
   },
   "source": [
    "**(d) [5 points] Interpreting rankings.** We went ahead and computed Cramer's V values for the 171 co-occurrence tables. Here are the pairs of drugs achieving the *lowest* 15 Cramer's V values:\n",
    "\n",
    "```\n",
    "Drug pair : Cramer's V value\n",
    "('chocolate', 'volatile substance abuse') : 0.006853233165433918\n",
    "('chocolate', 'methadone') : 0.007106981281108218\n",
    "('caffeine', 'semeron') : 0.007869948391973384\n",
    "('amyl nitrite', 'semeron') : 0.008160614641470642\n",
    "('benzodiazepine', 'chocolate') : 0.008425059464745031\n",
    "('chocolate', 'semeron') : 0.008579266066327003\n",
    "('chocolate', 'mushrooms') : 0.010928706148444899\n",
    "('alcohol', 'methadone') : 0.011402044438358674\n",
    "('chocolate', 'heroin') : 0.014395893642663278\n",
    "('chocolate', 'LSD') : 0.015354214527568324\n",
    "('alcohol', 'volatile substance abuse') : 0.016680904709057108\n",
    "('chocolate', 'nicotine') : 0.017000064195845693\n",
    "('amphetamines', 'semeron') : 0.0186534878915554\n",
    "('chocolate', 'cocaine') : 0.019638690380047206\n",
    "('methadone', 'semeron') : 0.020289516922874635\n",
    "```\n",
    "\n",
    "Do these results make sense to you? How can you tell? (In other words, how would you interpret at least some of these pairs, and why does the interpretation make sense?) In answering these questions, please keep your answer to a maximum of four sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c56a84",
   "metadata": {
    "id": "1d7d83d5"
   },
   "source": [
    "**Your answer here (please be sure to explain your reasoning):** First off, Semeron is a fictituous drug, so we would expect that it shouldn't be used at all, so it is reassuring that we're seeing it show up a bunch for these drug pairs with low Cramer's V values (which suggests that these pairs of drugs are not all that interesting). Next, chocolate is showing up a bunch of times, which is largely unsurprising in that nearly everyone eats chocolate so chocolate is going to co-occur with everything, and in particular, chocolate usage is likely unrelated to usage of other drugs (put another way, chocolate assumption is probably close to being independent of nearly all the other drugs being tracked, so we would expect its Cramer's V value to be near 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf1cdf7",
   "metadata": {
    "id": "5e23da73"
   },
   "source": [
    "## Problem 2: A Grab Bag of True/False Questions (No Coding) [20 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683c4021",
   "metadata": {
    "id": "38f03c0b"
   },
   "source": [
    "**(a) [5 points]** In a learned low-dimensional PCA space, distance is relative (so that, for instance, moving 1 unit of distance along the positive direction of the first principal component could mean moving different amounts of distance in the original high-dimensional feature space)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceb00b5",
   "metadata": {
    "id": "d0469e93"
   },
   "source": [
    "**Your answer here (please indicate true or false, and regardless of which option you put, you must explain your answer to receive any credit):** False. Distance is not relative in PCA space as PCA is linear. Moving one unit of distance in PCA space always corresponds to moving exactly the same amount in the high-dimensional space, where how much it moves depends on the PCA components weight matrix -- but very importantly it does *not* depend on where the point is in PCA space. This is in sharp contrast to nonlinear dimensionality reduction methods like t-SNE where we saw in class how distance can be relative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809a74eb",
   "metadata": {
    "id": "a096cf7b"
   },
   "source": [
    "**(b) [5 points]** For Isomap, when the number of nearest neighbors is set so high that we have an edge between every possible pair of points, then we are then just computing PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a87982",
   "metadata": {
    "id": "7bcd7bdd"
   },
   "source": [
    "**Your answer here (please indicate true or false, and regardless of which option you put, you must explain your answer to receive any credit):** False. It's MDS and not PCA. Note that while there are special cases of MDS that relate to versions of PCA, in general MDS and PCA are *not* mathematically equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9498b8e2",
   "metadata": {
    "id": "9073be5f"
   },
   "source": [
    "**(c) [5 points]** k-means can only work well if the data are generated from a Gaussian mixture model where each cluster's shape is an equal-sized circle (or in higher dimensions, spheres/hyperspheres)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a21a024",
   "metadata": {
    "id": "07ff4bf5"
   },
   "source": [
    "**Your answer here (please indicate true or false, and regardless of which option you put, you must explain your answer to receive any credit):** False. Importantly, the statement starts with \"k-means can only work well if\"; we just need to show an example where k-means works well and the data aren't coming from a GMM with equal-sized circle-shaped clusters. In lecture, there was an example of the umbrella and triangle shapes that are well-separated that k-means would be able to work well on too. This phenomenon holds more generally: if you have very well-separated true clusters, then even if they are of different sizes and shapes, k-means can still work well in identifying these clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8acc21",
   "metadata": {
    "id": "758f30a5"
   },
   "source": [
    "**(d) [5 points]** Before running PCA, we should always standardize the features (per feature, subtract the mean and divide by the standard deviation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3683f67",
   "metadata": {
    "id": "24ee0c2b"
   },
   "source": [
    "**Your answer here (please indicate true or false, and regardless of which option you put, you must explain your answer to receive any credit):** False. The statement says that we \"should always standardize the features\"; the keyword here \"always\" makes the statement false. Whether we should standardize features really depends on the dataset. For example, if you have a dataset where for a specific feature, the standard deviation is actually 0, then you would run into a divide by 0 problem. Separately, you could have a feature that is very unimportant, then by standardizing, you're changing every feature to have zero mean and unit variance, which might over-emphasize the unimportant feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8adba86",
   "metadata": {
    "id": "11950a95"
   },
   "source": [
    "## Problem 3: Tweets on Covid (Coding) [60 points]\n",
    "\n",
    "In this problem, you will be working on a dataset of **tweets related to COVID-19**. These tweets have already been labeled with sentiment scores. We want to understand differences in what people talk about across different sentiments scores. Note that our goal here is explicitly *not* to solve any sort of prediction task (such as predicting sentiment scores). You could actually think of this problem as interpreting clusters of text documents, where someone has provided cluster labels for you (corresponding to sentiment scores).\n",
    "\n",
    "We begin with some imports. **Important: Your code should not be importing any additional packages.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a5113e7",
   "metadata": {
    "executionInfo": {
     "elapsed": 449,
     "status": "ok",
     "timestamp": 1637190618300,
     "user": {
      "displayName": "Yuan Pei Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05454922516143727626"
     },
     "user_tz": 300
    },
    "id": "80a8b5f5"
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL -- ** BE SURE TO RUN THIS CELL **\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, CountVectorizer\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2045f4c4",
   "metadata": {
    "id": "dd846528"
   },
   "source": [
    "Next, we load in tweets and sentiment scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12019b30",
   "metadata": {
    "id": "552c9598"
   },
   "outputs": [],
   "source": [
    "with open('mystery_shoe_box.txt', 'r', encoding='utf-8') as f:\n",
    "    tweets = []\n",
    "    for line in f.readlines():\n",
    "        tweets.append(line.strip())\n",
    "    tweets = np.array(tweets)\n",
    "\n",
    "with open('mystery_candy_corn.txt', 'r', encoding='utf-8') as f:\n",
    "    sentiments = []\n",
    "    for line in f.readlines():\n",
    "        sentiments.append(line.strip())\n",
    "    sentiments = np.array(sentiments)\n",
    "\n",
    "assert len(tweets) == len(sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2447da",
   "metadata": {
    "id": "c0bc0e9f"
   },
   "source": [
    "Note that the i-th tweet `tweets[i]` has sentiment score given by `sentiments[i]`. As an illustrative example, the below code prints out the first five tweets, prepended by their sentiment scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77db8c03",
   "metadata": {
    "executionInfo": {
     "elapsed": 255,
     "status": "aborted",
     "timestamp": 1637190618695,
     "user": {
      "displayName": "Yuan Pei Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05454922516143727626"
     },
     "user_tz": 300
    },
    "id": "9616f2df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extremely Negative : trending: new yorkers encounter empty supermarket shelves (pictured, wegmans in brooklyn), sold-out online grocers (foodkick, maxdelivery) as #coronavirus-fearing shoppers stock up https://t.co/gr76pcrlwh https://t.co/ivmkmsqdt1\n",
      "\n",
      "Positive : when i couldn't find hand sanitizer at fred meyer, i turned to #amazon. but $114.97 for a 2 pack of purell??!!check out how #coronavirus concerns are driving up prices. https://t.co/ygbipbflmy\n",
      "\n",
      "Extremely Positive : find out how you can protect yourself and loved ones from #coronavirus. ?\n",
      "\n",
      "Negative : #panic buying hits #newyork city as anxious shoppers stock up on food&amp;medical supplies after #healthcare worker in her 30s becomes #bigapple 1st confirmed #coronavirus patient or a #bloomberg staged event? https://t.co/iasiregpc4 #qanon #qanon2018 #qanon2020 #election2020 #cdc https://t.co/29iszoewxu\n",
      "\n",
      "Neutral : #toiletpaper #dunnypaper #coronavirus #coronavirusaustralia #coronavirusupdate #covid_19 #9news #corvid19 #7newsmelb #dunnypapergate #costco one week everyone buying baby milk powder the next everyone buying up toilet paper. https://t.co/sczryvvsih\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentiment, tweet in list(zip(sentiments, tweets))[:5]:\n",
    "    print(sentiment, ':', tweet)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743b8676",
   "metadata": {
    "id": "9057a5f1"
   },
   "source": [
    "**Important**: For this problem, parts (a), (b), (c), and (d) can be done in any order. If you get stuck, try attempting a different part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837972af",
   "metadata": {
    "id": "65f57658"
   },
   "source": [
    "**(a) [10 points total across subparts]** This is a warm-up problem, just to make sure you familiarize yourself with the data, and as a bit of basic Python coding review. \n",
    "\n",
    "**Subpart i [2.5 points].** How many tweets are tagged as \"Extremely Positive\"? In other words, how many \"Extremely Positive\" are there in the column of `Sentiment`? **Your Python code for this subpart should print out the answer to this question.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00d7e12f",
   "metadata": {
    "executionInfo": {
     "elapsed": 256,
     "status": "aborted",
     "timestamp": 1637190618696,
     "user": {
      "displayName": "Yuan Pei Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05454922516143727626"
     },
     "user_tz": 300
    },
    "id": "05f69ace"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "599"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print((sentiments == 'Extremely Positive').sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21fa155",
   "metadata": {
    "id": "f6b99178"
   },
   "source": [
    "**Subpart ii [2.5 points].** How many tweets are tagged as \"Extremely Negative\"? In other words, how many \"Extremely Negative\" are there in the column of `Sentiment`? **Your Python code for this subpart should print out the answer to this question.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66631ab1",
   "metadata": {
    "executionInfo": {
     "elapsed": 256,
     "status": "aborted",
     "timestamp": 1637190618696,
     "user": {
      "displayName": "Yuan Pei Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05454922516143727626"
     },
     "user_tz": 300
    },
    "id": "eb3bde77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "592\n"
     ]
    }
   ],
   "source": [
    "print((sentiments == 'Extremely Negative').sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fc4829",
   "metadata": {
    "id": "dcdd58e3"
   },
   "source": [
    "**Subpart iii [5 points].** If we map \"Extremely Negative\" to the number -2, \"Negative\" to -1, \"Neutral\" to 0, \"Positive\" to 1, and \"Extremely Positive\" to 2, then what is the average sentiment score for this tweet dataset? **Your Python code for this subpart should print out the answer to this question.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f7e86f1",
   "metadata": {
    "executionInfo": {
     "elapsed": 257,
     "status": "aborted",
     "timestamp": 1637190618697,
     "user": {
      "displayName": "Yuan Pei Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05454922516143727626"
     },
     "user_tz": 300
    },
    "id": "323a0c21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.021063717746182202\n"
     ]
    }
   ],
   "source": [
    "numerical_list = []\n",
    "for x in sentiments:\n",
    "    if x == 'Extremely Negative':\n",
    "        numerical_list.append(-2)\n",
    "    elif x == 'Negative':\n",
    "        numerical_list.append(-1)\n",
    "    elif x == 'Neutral':\n",
    "        numerical_list.append(0)\n",
    "    elif x == 'Positive':\n",
    "        numerical_list.append(1)\n",
    "    else:\n",
    "        numerical_list.append(2)\n",
    "print(np.mean(numerical_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff2901b",
   "metadata": {
    "id": "e55dfd46"
   },
   "source": [
    "**(b) [15 points total across subparts]** In this problem, we will look at tweets with sentiment \"Extremely Positive\" and \"Extremely Negative\". Specifically, we aim to understand whether tweets with extremely positive sentiment are longer (or shorter) than those with extremely negative sentiment. For simplicity, we measure the length of a tweet `x` by just `len(x)` (i.e., we just count the number of characters).\n",
    "\n",
    "**Subpart i [4 points].** What is the mean length of tweets with extremely positive sentiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ea08fe7",
   "metadata": {
    "executionInfo": {
     "elapsed": 257,
     "status": "aborted",
     "timestamp": 1637190618697,
     "user": {
      "displayName": "Yuan Pei Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05454922516143727626"
     },
     "user_tz": 300
    },
    "id": "ccc4b62c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236.90484140233724\n"
     ]
    }
   ],
   "source": [
    "tweet_lengths_extremely_positive = np.array([len(x) for x in tweets[sentiments == 'Extremely Positive']])\n",
    "print(tweet_lengths_extremely_positive.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4038e24d",
   "metadata": {
    "id": "97996c1b"
   },
   "source": [
    "What is the mean length of tweets with extremely negative sentiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fb6a85f",
   "metadata": {
    "executionInfo": {
     "elapsed": 256,
     "status": "aborted",
     "timestamp": 1637190618697,
     "user": {
      "displayName": "Yuan Pei Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05454922516143727626"
     },
     "user_tz": 300
    },
    "id": "9301b1bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230.84459459459458\n"
     ]
    }
   ],
   "source": [
    "tweet_lengths_extremely_negative = np.array([len(x) for x in tweets[sentiments == 'Extremely Negative']])\n",
    "print(tweet_lengths_extremely_negative.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c4454e",
   "metadata": {
    "id": "b101f05b"
   },
   "source": [
    "**Subpart ii [6 points].** However, looking at the averages only tells an incomplete story. Let's look at the actual distributions of tweet length.\n",
    "\n",
    "Plot a histogram showing the distribution of tweet lengths amont tweets with extremely positive sentiment. The x axis should correspond to tweet length with values sorted (smallest to largest tweet length) and the y-axis corresponds to raw counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d72e09aa",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1637190618698,
     "user": {
      "displayName": "Yuan Pei Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05454922516143727626"
     },
     "user_tz": 300
    },
    "id": "02d81040"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 286 artists>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "extremely_positive_distribution = Counter(tweet_lengths_extremely_positive)\n",
    "x_values = list(range(tweet_lengths_extremely_positive.min(),\n",
    "                      tweet_lengths_extremely_positive.max() + 1))\n",
    "plt.bar(x_values, [extremely_positive_distribution[x] for x in x_values])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84a0bcb",
   "metadata": {
    "id": "f28eb558"
   },
   "source": [
    "Now plot a histogram showing the distribution of tweet lengths amont tweets with extremely negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9c59a66",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1637190618698,
     "user": {
      "displayName": "Yuan Pei Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05454922516143727626"
     },
     "user_tz": 300
    },
    "id": "194aa4a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 268 artists>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "extremely_negative_distribution = Counter(tweet_lengths_extremely_negative)\n",
    "x_values = list(range(tweet_lengths_extremely_negative.min(),\n",
    "                      tweet_lengths_extremely_negative.max() + 1))\n",
    "plt.bar(x_values, [extremely_negative_distribution[x] for x in x_values])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d459da",
   "metadata": {
    "id": "2819ce26"
   },
   "source": [
    "**Subpart iii [3 points].** Keep in mind that these tweets are just a small subsample of the entire population of tweets that exist.\n",
    "\n",
    "Given your answers to the preceding parts, do you think that tweets with extremely positive sentiment actually are different than tweets with extremely negative sentiment? If so, how come? If not, why do you think they aren't all that different?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5deed8",
   "metadata": {
    "id": "a9ebda18"
   },
   "source": [
    "**Your answer here (please be sure to explain your reasoning):** The mean lengths aren't that different and the histograms don't look dramatically different either. Tweets are short, so there's only so much one can say. It's not surprising to see in both cases that there's a bit of a skew where many tweets are close to the maximum allowed by Twitter (280 characters)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5448baaf",
   "metadata": {
    "id": "bda418df"
   },
   "source": [
    "**Subpart iv [2 points].** Twitter limits tweets to 280 characters. Very briefly (no more than 2 sentences max) explain why you might be seeing tweets that are longer than 280 characters in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501b07dc",
   "metadata": {
    "id": "f6a2c6ff"
   },
   "source": [
    "**Your answer here (please be sure to explain your reasoning):** One way to answer this question is by actually writing some code and printing out what some tweets over length 280 look like. What you'll find is that there are some systematic artifacts that result from how the dataset was collected (e.g., threads get prepended text, some punctuation has been turned into extra spaces, etc). Basically there are many cases where there's actually new text added (such as some tag information) that isn't just explained by, for instance, the text encoding used, or special characters in the post."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85a672a",
   "metadata": {
    "id": "faf2d6fd"
   },
   "source": [
    "**(c) [15 points total across subparts]** Let's try to understand how extremely positive tweets differ from extremely negative tweets a different way: by looking at \n",
    "the top 20 most frequently occuring words for each of these two groups.\n",
    "\n",
    "**Subpart i [10 points].** For this problem, we shall make use of scikit-learn's built-in English stop words list. It is defined in the following Python variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a89eaa7",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1637190618698,
     "user": {
      "displayName": "Yuan Pei Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05454922516143727626"
     },
     "user_tz": 300
    },
    "id": "9d386f8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({'a',\n",
       "           'about',\n",
       "           'above',\n",
       "           'across',\n",
       "           'after',\n",
       "           'afterwards',\n",
       "           'again',\n",
       "           'against',\n",
       "           'all',\n",
       "           'almost',\n",
       "           'alone',\n",
       "           'along',\n",
       "           'already',\n",
       "           'also',\n",
       "           'although',\n",
       "           'always',\n",
       "           'am',\n",
       "           'among',\n",
       "           'amongst',\n",
       "           'amoungst',\n",
       "           'amount',\n",
       "           'an',\n",
       "           'and',\n",
       "           'another',\n",
       "           'any',\n",
       "           'anyhow',\n",
       "           'anyone',\n",
       "           'anything',\n",
       "           'anyway',\n",
       "           'anywhere',\n",
       "           'are',\n",
       "           'around',\n",
       "           'as',\n",
       "           'at',\n",
       "           'back',\n",
       "           'be',\n",
       "           'became',\n",
       "           'because',\n",
       "           'become',\n",
       "           'becomes',\n",
       "           'becoming',\n",
       "           'been',\n",
       "           'before',\n",
       "           'beforehand',\n",
       "           'behind',\n",
       "           'being',\n",
       "           'below',\n",
       "           'beside',\n",
       "           'besides',\n",
       "           'between',\n",
       "           'beyond',\n",
       "           'bill',\n",
       "           'both',\n",
       "           'bottom',\n",
       "           'but',\n",
       "           'by',\n",
       "           'call',\n",
       "           'can',\n",
       "           'cannot',\n",
       "           'cant',\n",
       "           'co',\n",
       "           'con',\n",
       "           'could',\n",
       "           'couldnt',\n",
       "           'cry',\n",
       "           'de',\n",
       "           'describe',\n",
       "           'detail',\n",
       "           'do',\n",
       "           'done',\n",
       "           'down',\n",
       "           'due',\n",
       "           'during',\n",
       "           'each',\n",
       "           'eg',\n",
       "           'eight',\n",
       "           'either',\n",
       "           'eleven',\n",
       "           'else',\n",
       "           'elsewhere',\n",
       "           'empty',\n",
       "           'enough',\n",
       "           'etc',\n",
       "           'even',\n",
       "           'ever',\n",
       "           'every',\n",
       "           'everyone',\n",
       "           'everything',\n",
       "           'everywhere',\n",
       "           'except',\n",
       "           'few',\n",
       "           'fifteen',\n",
       "           'fifty',\n",
       "           'fill',\n",
       "           'find',\n",
       "           'fire',\n",
       "           'first',\n",
       "           'five',\n",
       "           'for',\n",
       "           'former',\n",
       "           'formerly',\n",
       "           'forty',\n",
       "           'found',\n",
       "           'four',\n",
       "           'from',\n",
       "           'front',\n",
       "           'full',\n",
       "           'further',\n",
       "           'get',\n",
       "           'give',\n",
       "           'go',\n",
       "           'had',\n",
       "           'has',\n",
       "           'hasnt',\n",
       "           'have',\n",
       "           'he',\n",
       "           'hence',\n",
       "           'her',\n",
       "           'here',\n",
       "           'hereafter',\n",
       "           'hereby',\n",
       "           'herein',\n",
       "           'hereupon',\n",
       "           'hers',\n",
       "           'herself',\n",
       "           'him',\n",
       "           'himself',\n",
       "           'his',\n",
       "           'how',\n",
       "           'however',\n",
       "           'hundred',\n",
       "           'i',\n",
       "           'ie',\n",
       "           'if',\n",
       "           'in',\n",
       "           'inc',\n",
       "           'indeed',\n",
       "           'interest',\n",
       "           'into',\n",
       "           'is',\n",
       "           'it',\n",
       "           'its',\n",
       "           'itself',\n",
       "           'keep',\n",
       "           'last',\n",
       "           'latter',\n",
       "           'latterly',\n",
       "           'least',\n",
       "           'less',\n",
       "           'ltd',\n",
       "           'made',\n",
       "           'many',\n",
       "           'may',\n",
       "           'me',\n",
       "           'meanwhile',\n",
       "           'might',\n",
       "           'mill',\n",
       "           'mine',\n",
       "           'more',\n",
       "           'moreover',\n",
       "           'most',\n",
       "           'mostly',\n",
       "           'move',\n",
       "           'much',\n",
       "           'must',\n",
       "           'my',\n",
       "           'myself',\n",
       "           'name',\n",
       "           'namely',\n",
       "           'neither',\n",
       "           'never',\n",
       "           'nevertheless',\n",
       "           'next',\n",
       "           'nine',\n",
       "           'no',\n",
       "           'nobody',\n",
       "           'none',\n",
       "           'noone',\n",
       "           'nor',\n",
       "           'not',\n",
       "           'nothing',\n",
       "           'now',\n",
       "           'nowhere',\n",
       "           'of',\n",
       "           'off',\n",
       "           'often',\n",
       "           'on',\n",
       "           'once',\n",
       "           'one',\n",
       "           'only',\n",
       "           'onto',\n",
       "           'or',\n",
       "           'other',\n",
       "           'others',\n",
       "           'otherwise',\n",
       "           'our',\n",
       "           'ours',\n",
       "           'ourselves',\n",
       "           'out',\n",
       "           'over',\n",
       "           'own',\n",
       "           'part',\n",
       "           'per',\n",
       "           'perhaps',\n",
       "           'please',\n",
       "           'put',\n",
       "           'rather',\n",
       "           're',\n",
       "           'same',\n",
       "           'see',\n",
       "           'seem',\n",
       "           'seemed',\n",
       "           'seeming',\n",
       "           'seems',\n",
       "           'serious',\n",
       "           'several',\n",
       "           'she',\n",
       "           'should',\n",
       "           'show',\n",
       "           'side',\n",
       "           'since',\n",
       "           'sincere',\n",
       "           'six',\n",
       "           'sixty',\n",
       "           'so',\n",
       "           'some',\n",
       "           'somehow',\n",
       "           'someone',\n",
       "           'something',\n",
       "           'sometime',\n",
       "           'sometimes',\n",
       "           'somewhere',\n",
       "           'still',\n",
       "           'such',\n",
       "           'system',\n",
       "           'take',\n",
       "           'ten',\n",
       "           'than',\n",
       "           'that',\n",
       "           'the',\n",
       "           'their',\n",
       "           'them',\n",
       "           'themselves',\n",
       "           'then',\n",
       "           'thence',\n",
       "           'there',\n",
       "           'thereafter',\n",
       "           'thereby',\n",
       "           'therefore',\n",
       "           'therein',\n",
       "           'thereupon',\n",
       "           'these',\n",
       "           'they',\n",
       "           'thick',\n",
       "           'thin',\n",
       "           'third',\n",
       "           'this',\n",
       "           'those',\n",
       "           'though',\n",
       "           'three',\n",
       "           'through',\n",
       "           'throughout',\n",
       "           'thru',\n",
       "           'thus',\n",
       "           'to',\n",
       "           'together',\n",
       "           'too',\n",
       "           'top',\n",
       "           'toward',\n",
       "           'towards',\n",
       "           'twelve',\n",
       "           'twenty',\n",
       "           'two',\n",
       "           'un',\n",
       "           'under',\n",
       "           'until',\n",
       "           'up',\n",
       "           'upon',\n",
       "           'us',\n",
       "           'very',\n",
       "           'via',\n",
       "           'was',\n",
       "           'we',\n",
       "           'well',\n",
       "           'were',\n",
       "           'what',\n",
       "           'whatever',\n",
       "           'when',\n",
       "           'whence',\n",
       "           'whenever',\n",
       "           'where',\n",
       "           'whereafter',\n",
       "           'whereas',\n",
       "           'whereby',\n",
       "           'wherein',\n",
       "           'whereupon',\n",
       "           'wherever',\n",
       "           'whether',\n",
       "           'which',\n",
       "           'while',\n",
       "           'whither',\n",
       "           'who',\n",
       "           'whoever',\n",
       "           'whole',\n",
       "           'whom',\n",
       "           'whose',\n",
       "           'why',\n",
       "           'will',\n",
       "           'with',\n",
       "           'within',\n",
       "           'without',\n",
       "           'would',\n",
       "           'yet',\n",
       "           'you',\n",
       "           'your',\n",
       "           'yours',\n",
       "           'yourself',\n",
       "           'yourselves'})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENGLISH_STOP_WORDS  # DO NOT MODIFY THIS LINE OR THIS VARIABLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72739273",
   "metadata": {
    "id": "6cc14129"
   },
   "source": [
    "Next, construct a histogram of words and their raw count frequencies in tweets with extremely positive sentiment. To do this, use the `Counter` object, and for each tweet `x`, please tokenize the tweet by just using `x.split()`. For your histogram, make sure that the words in `ENGLISH_STOP_WORDS` are not included in your histogram (or if they show up in your histogram, their raw count is set to 0). Display the top 20 words in the histogram, along with their raw counts.\n",
    "\n",
    "**Warning:** do *not* use spaCy or `CountVectorizer`/`TfidfVectorizer`; if your solution uses either of these, you will not receive credit as we want to make sure you understand the basic mechanics of computing histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecf60b05",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1637190618699,
     "user": {
      "displayName": "Yuan Pei Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05454922516143727626"
     },
     "user_tz": 300
    },
    "id": "1bc8f5f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('#covid_19', 213)\n",
      "('#coronavirus', 192)\n",
      "('food', 144)\n",
      "('store', 127)\n",
      "('grocery', 124)\n",
      "('&amp;', 119)\n",
      "('stock', 116)\n",
      "('online', 90)\n",
      "('people', 89)\n",
      "('like', 87)\n",
      "('shopping', 87)\n",
      "('help', 78)\n",
      "('just', 63)\n",
      "('need', 63)\n",
      "('local', 58)\n",
      "('covid-19', 57)\n",
      "('hand', 55)\n",
      "('?', 54)\n",
      "('#covid19', 43)\n",
      "('prices', 43)\n"
     ]
    }
   ],
   "source": [
    "word_histogram_extremely_positive = Counter()\n",
    "for tweet in tweets[sentiments == 'Extremely Positive']:\n",
    "    word_histogram_extremely_positive.update(tweet.split())\n",
    "for stop_word in ENGLISH_STOP_WORDS:\n",
    "    word_histogram_extremely_positive[stop_word] = 0\n",
    "for k, v in word_histogram_extremely_positive.most_common()[:20]:\n",
    "    print((k, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48f3603",
   "metadata": {
    "id": "a5366e41"
   },
   "source": [
    "Now repeat the above for the tweets with extremely negative sentiment (i.e., print out the top 20 words for tweets with extremely negative sentiment, excluding the stop words in `ENGLISH_STOP_WORDS`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7adac9a",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1637190618699,
     "user": {
      "displayName": "Yuan Pei Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05454922516143727626"
     },
     "user_tz": 300
    },
    "id": "f2746f42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('#covid_19', 237)\n",
      "('food', 205)\n",
      "('#coronavirus', 191)\n",
      "('people', 176)\n",
      "('panic', 152)\n",
      "('&amp;', 112)\n",
      "('grocery', 106)\n",
      "('stock', 105)\n",
      "('store', 101)\n",
      "('buying', 84)\n",
      "('toilet', 83)\n",
      "('just', 78)\n",
      "('prices', 72)\n",
      "('need', 70)\n",
      "('covid-19', 67)\n",
      "('supermarket', 53)\n",
      "('buy', 49)\n",
      "('going', 49)\n",
      "('shopping', 47)\n",
      "('stop', 45)\n"
     ]
    }
   ],
   "source": [
    "word_histogram_extremely_positive = Counter()\n",
    "for tweet in tweets[sentiments == 'Extremely Negative']:\n",
    "    word_histogram_extremely_positive.update(tweet.split())\n",
    "for stop_word in ENGLISH_STOP_WORDS:\n",
    "    word_histogram_extremely_positive[stop_word] = 0\n",
    "for k, v in word_histogram_extremely_positive.most_common()[:20]:\n",
    "    print((k, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b3f716",
   "metadata": {
    "id": "31d3e365"
   },
   "source": [
    "**Subpart ii. [5 points]** Find two words (note: there could be more than two) among the top 20 for extremely negative tweets that are *not* among the top 20 for extremely positive tweets. State what these two words are, and why you think they might be showing up a lot for extremely negative tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9239edef",
   "metadata": {
    "id": "e937f038"
   },
   "source": [
    "**Your answer here:** \"Panic\" and \"toilet\" are two examples. These of course correspond to negative sentiment during the pandemic, as there was a lot of panic (leading to people hoarding supplies at grocery stores, for instance), and also there was often an extremely limited supply of toilet paper. Both of these were situations that people were generally not all that happy about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bf3494",
   "metadata": {
    "id": "1ba9ea36"
   },
   "source": [
    "**(d) [20 points total across subparts]** Let's try to understand how extremely positive tweets differ from extremely negative tweets with the help of co-occurrence analysis. Specifically, we look at co-occurrences of *words*. Basically the co-occurrence table in this case would have rows and columns both be over the same words! For simplicity, we define the co-occurrence of a word with itself as zero.\n",
    "\n",
    "We have provided code to compute a PMI table for you. Do not modify this piece of code, but please do run it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12582f11",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1637190618699,
     "user": {
      "displayName": "Yuan Pei Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05454922516143727626"
     },
     "user_tz": 300
    },
    "id": "5de9c9fd"
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL -- ** BE SURE TO RUN THIS CELL **\n",
    "\n",
    "def get_PMI(list_of_tweets):\n",
    "    count_model = CountVectorizer(min_df=0.005)\n",
    "    X = count_model.fit_transform(list_of_tweets)\n",
    "    X[X > 0] = 1 \n",
    "    Xc = (X.T * X)\n",
    "    Xc.setdiag(0)\n",
    "    joint_prob_table = Xc / Xc.sum()\n",
    "    x_prob = joint_prob_table.sum(axis=1)\n",
    "    y_prob = joint_prob_table.sum(axis=0)\n",
    "    joint_prob_table_indep = np.outer(x_prob, y_prob)\n",
    "    return np.log2(joint_prob_table / joint_prob_table_indep), count_model.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a1ae3e",
   "metadata": {
    "id": "cb294005"
   },
   "source": [
    "What the function `get_PMI()` does is that given a list of Python strings (each string represents a different text document), it computes a co-occurrence table in terms of how many documents mention each pair of words, and then it computes a PMI table based on this co-occurrence table.\n",
    "\n",
    "As an illustrative example, we have the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8aade3b4",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1637190618699,
     "user": {
      "displayName": "Yuan Pei Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05454922516143727626"
     },
     "user_tz": 300
    },
    "id": "0d04587c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-9d72f369e5a3>:13: RuntimeWarning: divide by zero encountered in log2\n",
      "  return np.log2(joint_prob_table / joint_prob_table_indep), count_model.get_feature_names()\n"
     ]
    }
   ],
   "source": [
    "toy_pmi_table, toy_vocab = get_PMI(['apple apple apple dog', 'apple dog', 'donkey apple'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "395339e2",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1637190618699,
     "user": {
      "displayName": "Yuan Pei Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05454922516143727626"
     },
     "user_tz": 300
    },
    "id": "b97c14fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple', 'dog', 'donkey']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dcfadf35",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1637190618700,
     "user": {
      "displayName": "Yuan Pei Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05454922516143727626"
     },
     "user_tz": 300
    },
    "id": "8e5a2898"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-inf,   1.,   1.],\n",
       "        [  1., -inf, -inf],\n",
       "        [  1., -inf, -inf]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_pmi_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75ceb1e",
   "metadata": {
    "id": "58d3db01"
   },
   "source": [
    "Here, the 0-th row corresponds to `'apple'`, the 1st row corresponds to `'dog'`, and the 2nd row corresponds to `'donkey'`. Similarly, the columns correspond to `'apple'`, `'dog'`, and `'donkey'` respectively. Because we defined the co-occurrence of a word with itself as 0, once we take log, the diagonal entries are all `-inf`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89473ee8",
   "metadata": {
    "id": "2b213ed8"
   },
   "source": [
    "**Subpart i [15 points].** Apply `get_PMI` to the tweets with extremely positive sentiment. Then list the top 20 word pairs with the highest PMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6b47fd2",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1637190618700,
     "user": {
      "displayName": "Yuan Pei Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05454922516143727626"
     },
     "user_tz": 300
    },
    "id": "cff2e0f3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-9d72f369e5a3>:13: RuntimeWarning: divide by zero encountered in log2\n",
      "  return np.log2(joint_prob_table / joint_prob_table_indep), count_model.get_feature_names()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.6511557104327945 ('doctors', 'nurses')\n",
      "7.521872693487829 ('enhanced', 'funds')\n",
      "7.461331151552779 ('funds', 'unemployment')\n",
      "7.446797211926611 ('insights', 'survey')\n",
      "7.440572591385981 ('enhanced', 'unemployment')\n",
      "7.419830164326341 ('insights', 'wellness')\n",
      "7.381114357210401 ('continues', 'increasingly')\n",
      "7.224890955730698 ('500', 'package')\n",
      "7.201737880423516 ('college', 'students')\n",
      "7.1974377429898935 ('growing', 'insights')\n",
      "7.196960740161647 ('baby', 'break')\n",
      "7.183668293004186 ('500', 'set')\n",
      "7.159302614103121 ('funds', 'testing')\n",
      "7.138544053936323 ('enhanced', 'testing')\n",
      "7.104596722012987 ('500', 'hi')\n",
      "7.104596722012985 ('coronapocolypse', 'coronavirusupdates')\n",
      "7.078002512001271 ('testing', 'unemployment')\n",
      "7.0740553274846105 ('body', 'wellness')\n",
      "7.045434649544842 ('drink', 'insights')\n",
      "7.044147103684381 ('click', 'india')\n"
     ]
    }
   ],
   "source": [
    "pmi_table_extremely_positive, vocab_extremely_positive = get_PMI(tweets[sentiments == 'Extremely Positive'])\n",
    "n_vocab_extremely_positive = len(vocab_extremely_positive)\n",
    "list_to_sort = [(pmi_table_extremely_positive[i, j], (vocab_extremely_positive[i], vocab_extremely_positive[j]))\n",
    "                for i in range(1, n_vocab_extremely_positive)\n",
    "                for j in range(i + 1, n_vocab_extremely_positive)]\n",
    "sorted_list = sorted(list_to_sort, reverse=True, key=lambda x: x[0])\n",
    "for pmi, pair in sorted_list[:20]:\n",
    "    print(pmi, pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29020f95",
   "metadata": {
    "id": "c695e089"
   },
   "source": [
    "Now, apply `get_PMI` to the tweets with extremely negative sentiment and list the top 20 word pairs with the highest PMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "48c5e4c8",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1637190618700,
     "user": {
      "displayName": "Yuan Pei Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05454922516143727626"
     },
     "user_tz": 300
    },
    "id": "e7fcd8aa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-9d72f369e5a3>:13: RuntimeWarning: divide by zero encountered in log2\n",
      "  return np.log2(joint_prob_table / joint_prob_table_indep), count_model.get_feature_names()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.315518876478556 ('airlines', 'bookings')\n",
      "8.315518876478556 ('airlines', 'coz')\n",
      "8.315518876478556 ('airlines', 'plane')\n",
      "8.315518876478556 ('airlines', 'ticket')\n",
      "8.315518876478556 ('bookings', 'coz')\n",
      "8.315518876478556 ('bookings', 'plane')\n",
      "8.315518876478556 ('bookings', 'ticket')\n",
      "8.315518876478556 ('coz', 'plane')\n",
      "8.315518876478556 ('coz', 'ticket')\n",
      "8.315518876478556 ('plane', 'ticket')\n",
      "7.5875984219153585 ('dropping', 'plane')\n",
      "7.5875984219153585 ('dropping', 'ticket')\n",
      "7.587598421915357 ('airlines', 'dropping')\n",
      "7.587598421915357 ('bookings', 'dropping')\n",
      "7.587598421915357 ('coz', 'dropping')\n",
      "7.4296318847811005 ('emerging', 'pathogens')\n",
      "7.4296318847811005 ('emerging', 'socialized')\n",
      "7.4296318847811005 ('kept', 'pathogens')\n",
      "7.4296318847811005 ('kept', 'socialized')\n",
      "7.4296318847811005 ('pathogens', 'socialized')\n"
     ]
    }
   ],
   "source": [
    "pmi_table_extremely_negative, vocab_extremely_negative = get_PMI(tweets[sentiments == 'Extremely Negative'])\n",
    "n_vocab_extremely_negative = len(vocab_extremely_negative)\n",
    "list_to_sort = [(pmi_table_extremely_negative[i, j], (vocab_extremely_negative[i], vocab_extremely_negative[j]))\n",
    "                for i in range(1, n_vocab_extremely_negative)\n",
    "                for j in range(i + 1, n_vocab_extremely_negative)]\n",
    "sorted_list = sorted(list_to_sort, reverse=True, key=lambda x: x[0])\n",
    "for pmi, pair in sorted_list[:20]:\n",
    "    print(pmi, pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c255c00",
   "metadata": {
    "id": "530d0d0e"
   },
   "source": [
    "**Subpart ii [5 points].** Briefly explain whether the PMI results make sense in this case in helping us explain extremely positive vs extremely negative sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf730ab",
   "metadata": {
    "id": "2ede7db7"
   },
   "source": [
    "**Your answer here:** The PMI results do seem to make some sense here. For tweets of positive sentiment, doctors and nurses are mentioned which make sense as they were regularly praised for being in the front lines during the covid pandemic. There are also some co-occurrences that seem to be about funding or testing (likely additional funding and additional testing are related to positive sentiment).\n",
    "\n",
    "For tweets of negative sentiment, we have lots of flight issues (airlines had lots of problems during the pandemic as there were very few travelers, lots of flights were cancelled due to travel restrictions, etc)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "UDA Fall 2021 Quiz 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
