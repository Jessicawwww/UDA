{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2ca16d4",
   "metadata": {
    "id": "5cc8a4b0"
   },
   "source": [
    "# CMU 95-865 - Unstructured Data Analytics Fall 2021 Quiz 1\n",
    "\n",
    "This is an 80 minute exam. We will only grade what is submitted via Canvas.\n",
    "\n",
    "You must fill in your name and your Andrew ID for this quiz to be graded. Moreover, filling out your name and Andrew ID below will serve as your agreement with us, the course staff, that you did not collaborate with anyone on this exam and that what you submit is truly your own individual work and not that of anyone else. Violations found will result in severe penalties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c915895",
   "metadata": {
    "id": "4084baf4"
   },
   "source": [
    "Your name:\n",
    "\n",
    "Your Andrew ID:\n",
    "\n",
    "**Warning: If you leave the above blank, your quiz will not be graded.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa94da8",
   "metadata": {
    "id": "18af11b8"
   },
   "source": [
    "**Important:** There are 3 problems that can be done in any order. The very first part of Problem 1 and most of Problem 3 involve coding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2352e0e",
   "metadata": {
    "id": "a88b1083"
   },
   "source": [
    "## Problem 1: Drug Consumption, Revisited (1 very brief coding part, and 3 non-coding parts) [20 points]\n",
    "\n",
    "Remember how in the clustering lectures, we analyzed the [UCI drug consumption dataset](https://archive.ics.uci.edu/ml/datasets/Drug+consumption+%28quantified%29)? We now revisit that dataset, except looking only at the features that correspond to what drugs people took. We specifically want to better understand patterns in the usage of different pairs of drugs. For instance, people who use one drug (such as amphetamines) might be more likely to use another drug (such as cannabis).\n",
    "\n",
    "There are a total of 18 real drugs that the study tracked, and also 1 fake drug (called Semeron) that the study included (basically it was added as a sanity check to see if some people just over-claimed what drugs they took, even claiming to take a fictituous drug). Thus, there are 19 drugs that we keep track of. For each respondent in the dataset, we keep track of whether they took each of the drugs or not (so that each feature is binary). Note that in the original dataset, they actually also keep track of when was the last time each respondent took each drug, if they claimed to have taken the drug--we will *not* use this information and will only be looking at whether they ever took the drug or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dd8f65",
   "metadata": {
    "id": "73722f15"
   },
   "source": [
    "**(a)  [5 points] Calculating phi-squared values.** As a warm-up to the later questions, please compute the phi-squared values for each of the following two co-occurrence tables (these are based on the real data!):\n",
    "\n",
    "```\n",
    "+---------------+-------------------+------------------+\n",
    "|               |   chocolate : yes |   chocolate : no |\n",
    "+===============+===================+==================+\n",
    "| alcohol : yes |              1822 |               29 |\n",
    "+---------------+-------------------+------------------+\n",
    "| alcohol : no  |                31 |                3 |\n",
    "+---------------+-------------------+------------------+\n",
    "\n",
    "+---------------+----------------------------------+---------------------------------+\n",
    "|               |   volatile substance abuse : yes |   volatile substance abuse : no |\n",
    "+===============+==================================+=================================+\n",
    "| ecstasy : yes |                              317 |                             547 |\n",
    "+---------------+----------------------------------+---------------------------------+\n",
    "| ecstasy : no  |                              113 |                             908 |\n",
    "+---------------+----------------------------------+---------------------------------+\n",
    "```\n",
    "\n",
    "Note that the numbers in the table correspond to the number of participants who fall into that particular box (e.g., 29 respondents have had alcohol but never had chocolate in their lives)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6018bbb",
   "metadata": {
    "id": "de2f600c"
   },
   "source": [
    "**Phi-squared value for alcohol & chocolate:** REPLACE THIS TEXT WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e24fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE FOR COMPUTING THE PHI-SQUARED VALUE FOR ALCOHOL & CHOCOLATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201ec38e",
   "metadata": {
    "id": "4353e395"
   },
   "source": [
    "**Phi-squared value for ectasy & volatile substance abuse:** REPLACE THIS TEXT WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830abc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE FOR COMPUTING THE PHI-SQUARED VALUE FOR ECTASY & VOLATILE SUBSTANCE ABUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe4385b",
   "metadata": {
    "id": "202660e3"
   },
   "source": [
    "---\n",
    "\n",
    "Of course, there aren't just 2 tables. Since there are 19 drugs, there are a total of $\\frac{19\\times18}{2} = 171$ co-occurrence tables (each of these are 2-by-2) to consider! We aren't going to ask you to compute the phi-squared values of all of these by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcce7e43",
   "metadata": {
    "id": "ae734242"
   },
   "source": [
    "**(b) [5 points] Max phi-squared value.** What is the maximum possible phi-squared value for 2-by-2 co-occurrence tables like the ones above (where each row and each column has a nonzero sum)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacd5f69",
   "metadata": {
    "id": "cca74913"
   },
   "source": [
    "**Your answer here (please be sure to include an explanation of how you got your answer):** REPLACE THIS WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9f485a",
   "metadata": {
    "id": "70136b34"
   },
   "source": [
    "**(c) [5 points] Ranking pairs of drugs.** True or false: to rank the 171 drug pairs, we cannot use phi-squared or Chi-squared, and instead, we have to use Cramer's V."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2167f7",
   "metadata": {
    "id": "bdd4bc05"
   },
   "source": [
    "**Your answer here (please indicate true or false, and regardless of which option you put, you must explain your answer to receive any credit):** REPLACE THIS WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190da8c9",
   "metadata": {
    "id": "47d8cde1"
   },
   "source": [
    "**(d) [5 points] Interpreting rankings.** We went ahead and computed Cramer's V values for the 171 co-occurrence tables. Here are the pairs of drugs achieving the *lowest* 15 Cramer's V values:\n",
    "\n",
    "```\n",
    "Drug pair : Cramer's V value\n",
    "('chocolate', 'volatile substance abuse') : 0.006853233165433918\n",
    "('chocolate', 'methadone') : 0.007106981281108218\n",
    "('caffeine', 'semeron') : 0.007869948391973384\n",
    "('amyl nitrite', 'semeron') : 0.008160614641470642\n",
    "('benzodiazepine', 'chocolate') : 0.008425059464745031\n",
    "('chocolate', 'semeron') : 0.008579266066327003\n",
    "('chocolate', 'mushrooms') : 0.010928706148444899\n",
    "('alcohol', 'methadone') : 0.011402044438358674\n",
    "('chocolate', 'heroin') : 0.014395893642663278\n",
    "('chocolate', 'LSD') : 0.015354214527568324\n",
    "('alcohol', 'volatile substance abuse') : 0.016680904709057108\n",
    "('chocolate', 'nicotine') : 0.017000064195845693\n",
    "('amphetamines', 'semeron') : 0.0186534878915554\n",
    "('chocolate', 'cocaine') : 0.019638690380047206\n",
    "('methadone', 'semeron') : 0.020289516922874635\n",
    "```\n",
    "\n",
    "Do these results make sense to you? How can you tell? (In other words, how would you interpret at least some of these pairs, and why does the interpretation make sense?) In answering these questions, please keep your answer to a maximum of four sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c56a84",
   "metadata": {
    "id": "1d7d83d5"
   },
   "source": [
    "**Your answer here (please be sure to explain your reasoning):** REPLACE THIS WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf1cdf7",
   "metadata": {
    "id": "5e23da73"
   },
   "source": [
    "## Problem 2: A Grab Bag of True/False Questions (No Coding) [20 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683c4021",
   "metadata": {
    "id": "38f03c0b"
   },
   "source": [
    "**(a) [5 points]** In a learned low-dimensional PCA space, distance is relative (so that, for instance, moving 1 unit of distance along the positive direction of the first principal component could mean moving different amounts of distance in the original high-dimensional feature space)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceb00b5",
   "metadata": {
    "id": "d0469e93"
   },
   "source": [
    "**Your answer here (please indicate true or false, and regardless of which option you put, you must explain your answer to receive any credit):** REPLACE THIS WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809a74eb",
   "metadata": {
    "id": "a096cf7b"
   },
   "source": [
    "**(b) [5 points]** For Isomap, when the number of nearest neighbors is set so high that we have an edge between every possible pair of points, then we are then just computing PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a87982",
   "metadata": {
    "id": "7bcd7bdd"
   },
   "source": [
    "**Your answer here (please indicate true or false, and regardless of which option you put, you must explain your answer to receive any credit):** REPLACE THIS WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9498b8e2",
   "metadata": {
    "id": "9073be5f"
   },
   "source": [
    "**(c) [5 points]** k-means can only work well if the data are generated from a Gaussian mixture model where each cluster's shape is an equal-sized circle (or in higher dimensions, spheres/hyperspheres)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a21a024",
   "metadata": {
    "id": "07ff4bf5"
   },
   "source": [
    "**Your answer here (please indicate true or false, and regardless of which option you put, you must explain your answer to receive any credit):** REPLACE THIS WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8acc21",
   "metadata": {
    "id": "758f30a5"
   },
   "source": [
    "**(d) [5 points]** Before running PCA, we should always standardize the features (per feature, subtract the mean and divide by the standard deviation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3683f67",
   "metadata": {
    "id": "24ee0c2b"
   },
   "source": [
    "**Your answer here (please indicate true or false, and regardless of which option you put, you must explain your answer to receive any credit):** REPLACE THIS WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8adba86",
   "metadata": {
    "id": "11950a95"
   },
   "source": [
    "## Problem 3: Tweets on Covid (Coding) [60 points]\n",
    "\n",
    "In this problem, you will be working on a dataset of **tweets related to COVID-19**. These tweets have already been labeled with sentiment scores. We want to understand differences in what people talk about across different sentiments scores. Note that our goal here is explicitly *not* to solve any sort of prediction task (such as predicting sentiment scores). You could actually think of this problem as interpreting clusters of text documents, where someone has provided cluster labels for you (corresponding to sentiment scores).\n",
    "\n",
    "We begin with some imports. **Important: Your code should not be importing any additional packages.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5113e7",
   "metadata": {
    "executionInfo": {
     "elapsed": 449,
     "status": "ok",
     "timestamp": 1637190618300,
     "user": {
      "displayName": "Yuan Pei Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05454922516143727626"
     },
     "user_tz": 300
    },
    "id": "80a8b5f5"
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL -- ** BE SURE TO RUN THIS CELL **\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, CountVectorizer\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2045f4c4",
   "metadata": {
    "id": "dd846528"
   },
   "source": [
    "Next, we load in tweets and sentiment scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12019b30",
   "metadata": {
    "id": "552c9598"
   },
   "outputs": [],
   "source": [
    "with open('mystery_shoe_box.txt', 'r', encoding='utf-8') as f:\n",
    "    tweets = []\n",
    "    for line in f.readlines():\n",
    "        tweets.append(line.strip())\n",
    "    tweets = np.array(tweets)\n",
    "\n",
    "with open('mystery_candy_corn.txt', 'r', encoding='utf-8') as f:\n",
    "    sentiments = []\n",
    "    for line in f.readlines():\n",
    "        sentiments.append(line.strip())\n",
    "    sentiments = np.array(sentiments)\n",
    "\n",
    "assert len(tweets) == len(sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2447da",
   "metadata": {
    "id": "c0bc0e9f"
   },
   "source": [
    "Note that the i-th tweet `tweets[i]` has sentiment score given by `sentiments[i]`. As an illustrative example, the below code prints out the first five tweets, prepended by their sentiment scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77db8c03",
   "metadata": {
    "executionInfo": {
     "elapsed": 255,
     "status": "aborted",
     "timestamp": 1637190618695,
     "user": {
      "displayName": "Yuan Pei Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05454922516143727626"
     },
     "user_tz": 300
    },
    "id": "9616f2df"
   },
   "outputs": [],
   "source": [
    "for sentiment, tweet in list(zip(sentiments, tweets))[:5]:\n",
    "    print(sentiment, ':', tweet)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743b8676",
   "metadata": {
    "id": "9057a5f1"
   },
   "source": [
    "**Important**: For this problem, parts (a), (b), (c), and (d) can be done in any order. If you get stuck, try attempting a different part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837972af",
   "metadata": {
    "id": "65f57658"
   },
   "source": [
    "**(a) [10 points total across subparts]** This is a warm-up problem, just to make sure you familiarize yourself with the data, and as a bit of basic Python coding review. \n",
    "\n",
    "**Subpart i [2.5 points].** How many tweets are tagged as \"Extremely Positive\"? In other words, how many \"Extremely Positive\" are there in the column of `Sentiment`? **Your Python code for this subpart should print out the answer to this question.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d7e12f",
   "metadata": {
    "executionInfo": {
     "elapsed": 256,
     "status": "aborted",
     "timestamp": 1637190618696,
     "user": {
      "displayName": "Yuan Pei Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05454922516143727626"
     },
     "user_tz": 300
    },
    "id": "05f69ace"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21fa155",
   "metadata": {
    "id": "f6b99178"
   },
   "source": [
    "**Subpart ii [2.5 points].** How many tweets are tagged as \"Extremely Negative\"? In other words, how many \"Extremely Negative\" are there in the column of `Sentiment`? **Your Python code for this subpart should print out the answer to this question.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66631ab1",
   "metadata": {
    "executionInfo": {
     "elapsed": 256,
     "status": "aborted",
     "timestamp": 1637190618696,
     "user": {
      "displayName": "Yuan Pei Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05454922516143727626"
     },
     "user_tz": 300
    },
    "id": "eb3bde77"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fc4829",
   "metadata": {
    "id": "dcdd58e3"
   },
   "source": [
    "**Subpart iii [5 points].** If we map \"Extremely Negative\" to the number -2, \"Negative\" to -1, \"Neutral\" to 0, \"Positive\" to 1, and \"Extremely Positive\" to 2, then what is the average sentiment score for this tweet dataset? **Your Python code for this subpart should print out the answer to this question.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7e86f1",
   "metadata": {
    "executionInfo": {
     "elapsed": 257,
     "status": "aborted",
     "timestamp": 1637190618697,
     "user": {
      "displayName": "Yuan Pei Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05454922516143727626"
     },
     "user_tz": 300
    },
    "id": "323a0c21"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff2901b",
   "metadata": {
    "id": "e55dfd46"
   },
   "source": [
    "**(b) [15 points total across subparts]** In this problem, we will look at tweets with sentiment \"Extremely Positive\" and \"Extremely Negative\". Specifically, we aim to understand whether tweets with extremely positive sentiment are longer (or shorter) than those with extremely negative sentiment. For simplicity, we measure the length of a tweet `x` by just `len(x)` (i.e., we just count the number of characters).\n",
    "\n",
    "**Subpart i [4 points].** What is the mean length of tweets with extremely positive sentiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea08fe7",
   "metadata": {
    "executionInfo": {
     "elapsed": 257,
     "status": "aborted",
     "timestamp": 1637190618697,
     "user": {
      "displayName": "Yuan Pei Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05454922516143727626"
     },
     "user_tz": 300
    },
    "id": "ccc4b62c"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4038e24d",
   "metadata": {
    "id": "97996c1b"
   },
   "source": [
    "What is the mean length of tweets with extremely negative sentiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb6a85f",
   "metadata": {
    "executionInfo": {
     "elapsed": 256,
     "status": "aborted",
     "timestamp": 1637190618697,
     "user": {
      "displayName": "Yuan Pei Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05454922516143727626"
     },
     "user_tz": 300
    },
    "id": "9301b1bc"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c4454e",
   "metadata": {
    "id": "b101f05b"
   },
   "source": [
    "**Subpart ii [6 points].** However, looking at the averages only tells an incomplete story. Let's look at the actual distributions of tweet length.\n",
    "\n",
    "Plot a histogram showing the distribution of tweet lengths amont tweets with extremely positive sentiment. The x axis should correspond to tweet length with values sorted (smallest to largest tweet length) and the y-axis corresponds to raw counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72e09aa",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1637190618698,
     "user": {
      "displayName": "Yuan Pei Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05454922516143727626"
     },
     "user_tz": 300
    },
    "id": "02d81040"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84a0bcb",
   "metadata": {
    "id": "f28eb558"
   },
   "source": [
    "Now plot a histogram showing the distribution of tweet lengths amont tweets with extremely negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c59a66",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1637190618698,
     "user": {
      "displayName": "Yuan Pei Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05454922516143727626"
     },
     "user_tz": 300
    },
    "id": "194aa4a2"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d459da",
   "metadata": {
    "id": "2819ce26"
   },
   "source": [
    "**Subpart iii [3 points].** Keep in mind that these tweets are just a small subsample of the entire population of tweets that exist.\n",
    "\n",
    "Given your answers to the preceding parts, do you think that tweets with extremely positive sentiment actually are different than tweets with extremely negative sentiment? If so, how come? If not, why do you think they aren't all that different?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5deed8",
   "metadata": {
    "id": "a9ebda18"
   },
   "source": [
    "**Your answer here (please be sure to explain your reasoning):** REPLACE THIS WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5448baaf",
   "metadata": {
    "id": "bda418df"
   },
   "source": [
    "**Subpart iv [2 points].** Twitter limits tweets to 280 characters. Very briefly (no more than 2 sentences max) explain why you might be seeing tweets that are longer than 280 characters in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501b07dc",
   "metadata": {
    "id": "f6a2c6ff"
   },
   "source": [
    "**Your answer here (please be sure to explain your reasoning):** REPLACE THIS WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85a672a",
   "metadata": {
    "id": "faf2d6fd"
   },
   "source": [
    "**(c) [15 points total across subparts]** Let's try to understand how extremely positive tweets differ from extremely negative tweets a different way: by looking at the top 20 most frequently occuring words for each of these two groups.\n",
    "\n",
    "**Subpart i [10 points].** For this problem, we shall make use of scikit-learn's built-in English stop words list. It is defined in the following Python variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a89eaa7",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1637190618698,
     "user": {
      "displayName": "Yuan Pei Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05454922516143727626"
     },
     "user_tz": 300
    },
    "id": "9d386f8a"
   },
   "outputs": [],
   "source": [
    "ENGLISH_STOP_WORDS  # DO NOT MODIFY THIS LINE OR THIS VARIABLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72739273",
   "metadata": {
    "id": "6cc14129"
   },
   "source": [
    "Next, construct a histogram of words and their raw count frequencies in tweets with extremely positive sentiment. To do this, use the `Counter` object, and for each tweet `x`, please tokenize the tweet by just using `x.split()`. For your histogram, make sure that the words in `ENGLISH_STOP_WORDS` are not included in your histogram (or if they show up in your histogram, their raw count is set to 0). Display the top 20 words in the histogram, along with their raw counts.\n",
    "\n",
    "**Warning:** do *not* use spaCy or `CountVectorizer`/`TfidfVectorizer`; if your solution uses either of these, you will not receive credit as we want to make sure you understand the basic mechanics of computing histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf60b05",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1637190618699,
     "user": {
      "displayName": "Yuan Pei Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05454922516143727626"
     },
     "user_tz": 300
    },
    "id": "1bc8f5f0"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE (build histogram and print top 20 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48f3603",
   "metadata": {
    "id": "a5366e41"
   },
   "source": [
    "Now repeat the above for the tweets with extremely negative sentiment (i.e., print out the top 20 words for tweets with extremely negative sentiment, excluding the stop words in `ENGLISH_STOP_WORDS`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7adac9a",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1637190618699,
     "user": {
      "displayName": "Yuan Pei Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05454922516143727626"
     },
     "user_tz": 300
    },
    "id": "f2746f42"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE (build histogram and print top 20 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b3f716",
   "metadata": {
    "id": "31d3e365"
   },
   "source": [
    "**Subpart ii. [5 points]** Find two words (note: there could be more than two) among the top 20 for extremely negative tweets that are *not* among the top 20 for extremely positive tweets. State what these two words are, and why you think they might be showing up a lot for extremely negative tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9239edef",
   "metadata": {
    "id": "e937f038"
   },
   "source": [
    "**Your answer here:** REPLACE THIS WITH YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bf3494",
   "metadata": {
    "id": "1ba9ea36"
   },
   "source": [
    "**(d) [20 points total across subparts]** Let's try to understand how extremely positive tweets differ from extremely negative tweets with the help of co-occurrence analysis. Specifically, we look at co-occurrences of *words*. Basically the co-occurrence table in this case would have rows and columns both be over the same words! For simplicity, we define the co-occurrence of a word with itself as zero.\n",
    "\n",
    "We have provided code to compute a PMI table for you. Do not modify this piece of code, but please do run it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12582f11",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1637190618699,
     "user": {
      "displayName": "Yuan Pei Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05454922516143727626"
     },
     "user_tz": 300
    },
    "id": "5de9c9fd"
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL -- ** BE SURE TO RUN THIS CELL **\n",
    "\n",
    "def get_PMI(list_of_tweets):\n",
    "    count_model = CountVectorizer(min_df=0.005)\n",
    "    X = count_model.fit_transform(list_of_tweets)\n",
    "    X[X > 0] = 1 \n",
    "    Xc = (X.T * X)\n",
    "    Xc.setdiag(0)\n",
    "    joint_prob_table = Xc / Xc.sum()\n",
    "    x_prob = joint_prob_table.sum(axis=1)\n",
    "    y_prob = joint_prob_table.sum(axis=0)\n",
    "    joint_prob_table_indep = np.outer(x_prob, y_prob)\n",
    "    return np.log2(joint_prob_table / joint_prob_table_indep), count_model.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a1ae3e",
   "metadata": {
    "id": "cb294005"
   },
   "source": [
    "What the function `get_PMI()` does is that given a list of Python strings (each string represents a different text document), it computes a co-occurrence table in terms of how many documents mention each pair of words, and then it computes a PMI table based on this co-occurrence table.\n",
    "\n",
    "As an illustrative example, we have the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aade3b4",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1637190618699,
     "user": {
      "displayName": "Yuan Pei Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05454922516143727626"
     },
     "user_tz": 300
    },
    "id": "0d04587c"
   },
   "outputs": [],
   "source": [
    "toy_pmi_table, toy_vocab = get_PMI(['apple apple apple dog', 'apple dog', 'donkey apple'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395339e2",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1637190618699,
     "user": {
      "displayName": "Yuan Pei Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05454922516143727626"
     },
     "user_tz": 300
    },
    "id": "b97c14fc"
   },
   "outputs": [],
   "source": [
    "toy_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfadf35",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1637190618700,
     "user": {
      "displayName": "Yuan Pei Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05454922516143727626"
     },
     "user_tz": 300
    },
    "id": "8e5a2898"
   },
   "outputs": [],
   "source": [
    "toy_pmi_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75ceb1e",
   "metadata": {
    "id": "58d3db01"
   },
   "source": [
    "Here, the 0-th row corresponds to `'apple'`, the 1st row corresponds to `'dog'`, and the 2nd row corresponds to `'donkey'`. Similarly, the columns correspond to `'apple'`, `'dog'`, and `'donkey'` respectively. Because we defined the co-occurrence of a word with itself as 0, once we take log, the diagonal entries are all `-inf`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89473ee8",
   "metadata": {
    "id": "2b213ed8"
   },
   "source": [
    "**Subpart i [15 points].** Apply `get_PMI` to the tweets with extremely positive sentiment. Then list the top 20 word pairs with the highest PMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b47fd2",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1637190618700,
     "user": {
      "displayName": "Yuan Pei Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05454922516143727626"
     },
     "user_tz": 300
    },
    "id": "cff2e0f3"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE (list top 20 word pairs with the highest PMI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29020f95",
   "metadata": {
    "id": "c695e089"
   },
   "source": [
    "Now, apply `get_PMI` to the tweets with extremely negative sentiment and list the top 20 word pairs with the highest PMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c5e4c8",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1637190618700,
     "user": {
      "displayName": "Yuan Pei Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05454922516143727626"
     },
     "user_tz": 300
    },
    "id": "e7fcd8aa"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE (list top 20 word pairs with the highest PMI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c255c00",
   "metadata": {
    "id": "530d0d0e"
   },
   "source": [
    "**Subpart ii [5 points].** Briefly explain whether the PMI results make sense in this case in helping us explain extremely positive vs extremely negative sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf730ab",
   "metadata": {
    "id": "2ede7db7"
   },
   "source": [
    "**Your answer here:** REPLACE THIS WITH YOUR ANSWER"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "UDA Fall 2021 Quiz 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
